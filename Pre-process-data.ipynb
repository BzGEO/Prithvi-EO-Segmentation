{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3e40c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43d5815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal, osr\n",
    "from osgeo import gdal_array\n",
    "from osgeo import osr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import image_utils as iu\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import uuid\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e4b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'/content/drive/MyDrive/SCO_training'\n",
    "segmentation_path = os.path.join(data_path, 'segmentation')\n",
    "original_path = os.path.join(segmentation_path, 'original')\n",
    "real_data_path = os.path.join(original_path, 'real')\n",
    "# real_data_path = '/Volumes/BOOTCAMP/Users/eopok/Desktop/Sentinel/2022'\n",
    "mask_data_path = os.path.join(original_path, 'mask')\n",
    "# divided images\n",
    "divided_data_path = os.path.join(segmentation_path, 'divided')\n",
    "real_divided_data_path = os.path.join(divided_data_path, 'real')\n",
    "mask_divided_data_path = os.path.join(divided_data_path, 'mask')\n",
    "#\n",
    "final_data_path = os.path.join(segmentation_path, 'final')\n",
    "real_final_data_path = os.path.join(final_data_path, 'real')\n",
    "mask_final_data_path = os.path.join(final_data_path, 'mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bfafa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir_if_not_exists(path):\n",
    "    if os.path.exists(path):\n",
    "        return\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284dd0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dir_if_not_exists(data_path)\n",
    "create_dir_if_not_exists(segmentation_path)\n",
    "create_dir_if_not_exists(original_path)\n",
    "create_dir_if_not_exists(real_data_path)\n",
    "create_dir_if_not_exists(mask_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b4424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_file(url, dest_path, chunk_size=1024*1024):\n",
    "    \"\"\"\n",
    "    Download a large file from a URL with a progress bar.\n",
    "    Args:\n",
    "        url (str): File URL.\n",
    "        dest_path (str): Destination file path.\n",
    "        chunk_size (int): Download chunk size in bytes.\n",
    "    \"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    total = int(response.headers.get('content-length', 0))\n",
    "    with open(dest_path, 'wb') as file, tqdm(\n",
    "        desc=f\"Downloading {dest_path}\",\n",
    "        total=total,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for data in response.iter_content(chunk_size=chunk_size):\n",
    "            size = file.write(data)\n",
    "            bar.update(size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed14aa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "geotiff_image_path = os.path.join(real_data_path, 'sentinel_image.tif')\n",
    "download_file('https://sco-training.s3.us-east-2.amazonaws.com/sentinel_2015-0000011776-0000023552_harmonized.tif', geotiff_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58a4cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "geotiff_mask_path = os.path.join(mask_data_path, 'sentinel_mask.tif')\n",
    "download_file('https://sco-training.s3.us-east-2.amazonaws.com/sentinel_2015-0000011776-0000023552_harmonized.png', geotiff_mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaf5360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores the path of an image and a mask pair\n",
    "class DataLabelPair:\n",
    "    def __init__(self, data_real_path, label_path):\n",
    "        self.data_real_path = data_real_path\n",
    "        self.label_path = label_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad77ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets all mask files in the mask folder\n",
    "original_mask_images_path = iu.get_all_files(real_data_path, '*.tif')\n",
    "len(original_mask_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d7b53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_original = []\n",
    "for path in original_mask_images_path:\n",
    "    image = DataLabelPair(os.path.join(real_data_path, path),\n",
    "                          os.path.join(mask_data_path, path).replace('.tif', '.png'))\n",
    "    images_original.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a45013",
   "metadata": {},
   "outputs": [],
   "source": [
    "strides = 224, 224\n",
    "max_grid_size = 224, 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592c3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geoTiff_datatype(geoTiff):\n",
    "    \"\"\"\n",
    "    Get the GDAL data type of a GeoTIFF dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - geoTiff (gdal.Dataset): Input GeoTIFF dataset.\n",
    "\n",
    "    Returns:\n",
    "    - int: GDAL data type of the GeoTIFF dataset.\n",
    "\n",
    "    \"\"\"\n",
    "    band = geoTiff.GetRasterBand(1)\n",
    "    return band.DataType\n",
    "\n",
    "\n",
    "def get_geoTiff_numpy_datatype(geoTiff):\n",
    "    \"\"\"\n",
    "    Get the NumPy data type string of a GeoTIFF dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - geoTiff (gdal.Dataset): Input GeoTIFF dataset.\n",
    "\n",
    "    Returns:\n",
    "    - str: NumPy data type string of the GeoTIFF dataset.\n",
    "\n",
    "    \"\"\"\n",
    "    gt_dtype = get_geoTiff_datatype(geoTiff)\n",
    "    gdal_to_numpy_datatype = {\n",
    "        gdal.GDT_Byte: 'uint8',\n",
    "        gdal.GDT_UInt16: 'uint16',\n",
    "        gdal.GDT_Int16: 'int16',\n",
    "        gdal.GDT_UInt32: 'uint32',\n",
    "        gdal.GDT_Int32: 'int32',\n",
    "        gdal.GDT_Float32: 'float32',\n",
    "        gdal.GDT_Float64: 'float64'\n",
    "    }\n",
    "    numpy_datatype = gdal_to_numpy_datatype.get(gt_dtype, None)\n",
    "    return numpy_datatype\n",
    "\n",
    "\n",
    "def crop_geoTiff(geoTiff, left, top, right, bottom, dtype=None):\n",
    "    \"\"\"\n",
    "    Crop a GeoTIFF array to the specified region.\n",
    "\n",
    "    Parameters:\n",
    "    - geoTiff (gdal.Dataset): Input GeoTIFF dataset.\n",
    "    - left (int): Left coordinate of the crop region.\n",
    "    - top (int): Top coordinate of the crop region.\n",
    "    - right (int): Right coordinate of the crop region.\n",
    "    - bottom (int): Bottom coordinate of the crop region.\n",
    "    - dtype (str or None, optional): Desired data type of the output array. \n",
    "    If set to None, the data type is inferred from the band. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    - output (numpy.ndarray): Cropped array with dimensions (height, width, bands).\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If the crop dimensions exceed the size of the GeoTIFF.\n",
    "    \"\"\"\n",
    "    if (int(right) > geoTiff.RasterXSize) or (int(bottom) > geoTiff.RasterYSize):\n",
    "        # print(right, bottom)\n",
    "        # print(geoTiff.RasterXSize, geoTiff.RasterYSize)\n",
    "        raise ValueError('Crop dimensions exceed the size of the GeoTIFF.')\n",
    "    if dtype is None:\n",
    "        dtype = get_geoTiff_numpy_datatype(geoTiff)\n",
    "    width = abs(right - left)\n",
    "    height = abs(top - bottom)\n",
    "    output = np.zeros(\n",
    "        (int(height), int(width), geoTiff.RasterCount), dtype)\n",
    "    # bands = [None] * geoTiff.RasterCount\n",
    "    for x in range(geoTiff.RasterCount):\n",
    "        band = geoTiff.GetRasterBand(x + 1).ReadAsArray(left, top,\n",
    "                                                        int(width), int(height))\n",
    "        output[..., x] = band\n",
    "    return output\n",
    "\n",
    "\n",
    "def read_geoTiff_bands(geoTiff, bands=None, bbox=None, dtype='uint16'):\n",
    "    width = geoTiff.RasterXSize\n",
    "    height = geoTiff.RasterYSize\n",
    "    if bbox == None:\n",
    "       bbox = (0, width, 0, height)\n",
    "    left, right, top, bottom = bbox\n",
    "    bb_width = right - left\n",
    "    bb_height = bottom - top\n",
    "    band_list = []\n",
    "    if bands != None:\n",
    "        for val in bands:\n",
    "            band = geoTiff.GetRasterBand(val).ReadAsArray(left, top,\n",
    "                                                          int(bb_width), int(bb_height))\n",
    "            band_list.append(band)\n",
    "    else:\n",
    "        for x in range(geoTiff.RasterCount):\n",
    "            band = geoTiff.GetRasterBand(x + 1).ReadAsArray(left, top,\n",
    "                                                            int(bb_width), int(bb_height))\n",
    "            band_list.append(band)\n",
    "    output = np.zeros(\n",
    "        (int(bb_height), int(bb_width), len(band_list)), dtype)\n",
    "    for x in range(len(band_list)):\n",
    "        output[..., x] = band_list[x]\n",
    "    return output\n",
    "\n",
    "\n",
    "def read_image(path, color_space=None):\n",
    "    if color_space is None:\n",
    "        img = Image.open(path)\n",
    "    else:\n",
    "        img = Image.open(path).convert(color_space)\n",
    "    return img\n",
    "\n",
    "\n",
    "def split_geoTiff_image(geoTiff, max_grid_size, strides, dtype = 'uint16'):\n",
    "    parts = []\n",
    "    width, height = geoTiff.RasterXSize, geoTiff.RasterYSize\n",
    "    left = 0\n",
    "    top = 0\n",
    "    stride_width, stride_height = strides\n",
    "    max_grid_width, max_grid_height = max_grid_size\n",
    "    total_grids = np.ceil(height / stride_width) * np.ceil(width / stride_height)\n",
    "    for r in range(0, height, stride_height):\n",
    "        for c in range(0, width, stride_width):\n",
    "            left, top = c, r\n",
    "            right, bottom = min(\n",
    "                    c + max_grid_width, width), min(r + max_grid_height,height)\n",
    "            grid = crop_geoTiff(\n",
    "                    geoTiff, left, top, right, bottom, dtype)\n",
    "            # part = gu.crop_geoTiff(geoTiff, left, top, right, bottom)\n",
    "            parts.append(grid)\n",
    "    return parts\n",
    "\n",
    "\n",
    "def split_image(image, max_grid_size, strides):\n",
    "    width, height = image.size\n",
    "    left = 0\n",
    "    top = 0\n",
    "    stride_width, stride_height = strides\n",
    "    max_grid_width, max_grid_height = max_grid_size\n",
    "\n",
    "    for r in range(0, height, stride_height):\n",
    "        for c in range(0, width, stride_width):\n",
    "            left, top = c, r\n",
    "            right, bottom = min(c + max_grid_width, width), min(r + max_grid_height, height)\n",
    "            grid = image.crop((left, top, right, bottom))\n",
    "            yield grid  # Yield the grid as a generator\n",
    "\n",
    "\n",
    "def print_progress(current, total):\n",
    "    # Calculate the percentage of progress\n",
    "    progress = (current / total) * 100\n",
    "    # Print the progress bar\n",
    "    print(\"\\rProgress: [{0:50s}] {1:.1f}%\".format(\n",
    "        '#' * int(progress/2), progress), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4aea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "iu.create_dir_if_not_exists(mask_divided_data_path)\n",
    "iu.create_dir_if_not_exists(real_divided_data_path)\n",
    "\n",
    "for i, image_original in enumerate(images_original, 1):\n",
    "    real_image = gdal.Open(image_original.data_real_path)\n",
    "    np_real_image = read_geoTiff_bands(real_image, dtype='float32')\n",
    "    mask_image = read_image(image_original.label_path, 'L')\n",
    "    mask_image = mask_image.resize((real_image.RasterXSize, real_image.RasterYSize), Image.NEAREST)\n",
    "    \n",
    "    gt = real_image.GetGeoTransform()\n",
    "\n",
    "    filename = os.path.basename(image_original.data_real_path)\n",
    "    match = re.search(r'(\\d{4})', filename)\n",
    "    if match:\n",
    "        year = int(match.group(1))\n",
    "    else:\n",
    "        year = 2023\n",
    "\n",
    "    # Generate patches first to know their total number\n",
    "    real_patches = list(split_geoTiff_image(real_image, max_grid_size, strides, 'float32'))\n",
    "    mask_patches = list(split_image(mask_image, max_grid_size, strides))\n",
    "\n",
    "    grid_h = max((np_real_image.shape[1] - max_grid_size[0]) // strides[0] + 1, 1)\n",
    "    grid_w = max((np_real_image.shape[2] - max_grid_size[1]) // strides[1] + 1, 1)\n",
    "\n",
    "    for idx, (gt_image_part, mask_image_part) in enumerate(zip(real_patches, mask_patches)):\n",
    "        part_filename = str(uuid.uuid4())\n",
    "\n",
    "        h, w = gt_image_part.shape[1], gt_image_part.shape[2]\n",
    "\n",
    "        patch_row = idx // grid_w\n",
    "        patch_col = idx % grid_w\n",
    "        x_offset = patch_col * strides[1]\n",
    "        y_offset = patch_row * strides[0]\n",
    "\n",
    "        x_center = x_offset + w // 2\n",
    "        y_center = y_offset + h // 2\n",
    "\n",
    "        lon = gt[0] + x_center * gt[1] + y_center * gt[2]\n",
    "        lat = gt[3] + x_center * gt[4] + y_center * gt[5]\n",
    "\n",
    "        temporal_coords = np.array([[year, 1]], dtype=np.float32)\n",
    "        location_coords = np.array([lat, lon], dtype=np.float32)\n",
    "\n",
    "        # ---- CLASS REMAPPING ----\n",
    "        mask_image_part = mask_image_part.copy()\n",
    "        mask_image_part[np.isin(mask_image_part, [1, 2])] = 1\n",
    "        mask_image_part[mask_image_part == 3] = 2\n",
    "\n",
    "        np.savez(\n",
    "            os.path.join(real_divided_data_path, part_filename + '.npz'),\n",
    "            array1=gt_image_part,\n",
    "            temporal_coords=temporal_coords,\n",
    "            location_coords=location_coords\n",
    "        )\n",
    "        # Save the mask as npz\n",
    "        np.savez(\n",
    "            os.path.join(mask_divided_data_path, part_filename + '.npz'),\n",
    "            mask=mask_image_part\n",
    "        )\n",
    "        #mask_image_part.save(os.path.join(mask_divided_data_path, part_filename + '.png'))\n",
    "\n",
    "    print_progress(i, len(images_original))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409e9775",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Images that do not contain any information is deleted from storage.\n",
    "divided_images_path = iu.get_all_files(mask_divided_data_path)\n",
    "len(divided_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d081647",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_divided = []\n",
    "for path in divided_images_path:\n",
    "    image = DataLabelPair(os.path.join(real_divided_data_path, path.replace('.png', '.npz')),\n",
    "                          os.path.join(mask_divided_data_path, path))\n",
    "    images_divided.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0627f09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_numpy_file(path):\n",
    "    loaded = np.load(path, allow_pickle=True)\n",
    "    return loaded['array1']\n",
    "\n",
    "\n",
    "def display_np_geoTiff(np_geoTiff, rgb_bands, max = None):\n",
    "    r, g, b = rgb_bands\n",
    "    red_band = np_geoTiff[:, :, r]\n",
    "    green_band = np_geoTiff[:, :, g]\n",
    "    blue_band = np_geoTiff[:, :, b]\n",
    "    shape = np_geoTiff.shape\n",
    "    # max = np_geoTiff.max()\n",
    "    # np_rgb_image = np_geoTiff[:, :, 0:3][:, :, ::-1]\n",
    "    rgbOutput = np.zeros((shape[0], shape[1], 3), dtype=np_geoTiff.dtype)\n",
    "    rgbOutput[..., 0] = red_band\n",
    "    rgbOutput[..., 1] = green_band\n",
    "    rgbOutput[..., 2] = blue_band\n",
    "    if max is None:\n",
    "       max = rgbOutput.max()\n",
    "    if max == 0:\n",
    "       max = 1\n",
    "    rgbOutput = (rgbOutput / max) * 255\n",
    "    im = Image.fromarray(np.array(rgbOutput, dtype=np.uint8))\n",
    "    return im\n",
    "\n",
    "def display_legend(np_image, color_encoding):\n",
    "    flat_gh_im = np_image.reshape(-1, 1)\n",
    "    #\n",
    "    new_gh_im = [None] * flat_gh_im.shape[0]\n",
    "    for x in range(flat_gh_im.shape[0]):\n",
    "        new_gh_im[x] = color_encoding.get(flat_gh_im[x][0], [0, 0, 0])\n",
    "    #\n",
    "    new_gh_im = np.array(new_gh_im).reshape(\n",
    "        np_image.shape[0], np_image.shape[1], 3)\n",
    "    enc_im = Image.fromarray(np.array(new_gh_im, dtype=np.uint8))\n",
    "    return enc_im\n",
    "\n",
    "def overlay_mask(original_image, mask_image):\n",
    "    # Open the image and mask\n",
    "    image = original_image.convert('RGBA')\n",
    "    mask = mask_image.convert('RGBA')\n",
    "\n",
    "    # Make pixel value [0, 0, 0] transparent in the mask\n",
    "    mask_data = mask.getdata()\n",
    "    new_mask_data = []\n",
    "    for item in mask_data:\n",
    "        if item[:3] == (0, 0, 0):\n",
    "            # Set transparency for [0, 0, 0]\n",
    "            new_mask_data.append((0, 0, 0, 0))\n",
    "        else:\n",
    "            new_mask_data.append(item)\n",
    "    mask.putdata(new_mask_data)\n",
    "\n",
    "    # Overlay the mask on the image\n",
    "    overlaid = Image.alpha_composite(image, mask)\n",
    "\n",
    "    # Save the overlaid image\n",
    "    return overlaid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfc2660",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_color_encoding = {2: [180, 96, 0],  # Herbaecous Savanna\n",
    "                      1: [251, 72, 196],  # Open Mine\n",
    "                      0: [0, 0, 0]}  # No data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e17510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#20  12\n",
    "import random\n",
    "indx = random.randint(0, len(divided_images_path))\n",
    "np_gt = load_numpy_file(images_divided[indx].data_real_path) \n",
    "real_im = display_np_geoTiff(np_gt, (2, 1, 0))\n",
    "#\n",
    "np_gh_img = np.asarray(iu.read_image(\n",
    "    images_divided[indx].label_path))\n",
    "mask = display_legend(np_gh_img, new_color_encoding)\n",
    "display_images({'Satellite Image': real_im, 'Mask': mask,\n",
    "                'Overlay': overlay_mask(real_im, mask)}, (1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4260cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits dataset into training, validation and test sets.\n",
    "def train_val_test_split(val_per, test_per, input_data, labels=None):\n",
    "    if labels is not None and len(input_data) != len(labels):\n",
    "        raise Exception(\"input data and label length mismatch\")\n",
    "    data_len = len(input_data)\n",
    "    val_len = int(data_len * (val_per / 100))\n",
    "    test_len = int(data_len * (test_per / 100))\n",
    "    x_val = input_data[0: val_len]\n",
    "    x_test = input_data[val_len: val_len + test_len]\n",
    "    x_train = input_data[val_len + test_len: data_len]\n",
    "    if labels is not None:\n",
    "        y_val = labels[0: val_len]\n",
    "        y_test = labels[val_len: val_len + test_len]\n",
    "        y_train = labels[val_len + test_len: data_len]\n",
    "        return (x_train, y_train), (x_val, y_val), (x_test, y_test)\n",
    "    else:\n",
    "        return x_train, x_val, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089340b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, val_images, test_images = train_val_test_split(\n",
    "    15, 0, images_divided)\n",
    "print('Train length:', len(train_images))\n",
    "print('Val length:', len(val_images))\n",
    "print('Test length', len(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98620793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directories for all training data.\n",
    "training_path = os.path.join(segmentation_path, 'training')\n",
    "iu.create_dir_if_not_exists(training_path)\n",
    "#train\n",
    "train_path = os.path.join(training_path, 'train')\n",
    "iu.create_dir_if_not_exists(train_path)\n",
    "train_real_path = os.path.join(train_path, 'real')\n",
    "iu.create_dir_if_not_exists(train_real_path)\n",
    "train_mask_path = os.path.join(train_path, 'mask')\n",
    "iu.create_dir_if_not_exists(train_mask_path)\n",
    "#val\n",
    "val_path = os.path.join(training_path, 'val')\n",
    "iu.create_dir_if_not_exists(val_path)\n",
    "val_real_path = os.path.join(val_path, 'real')\n",
    "iu.create_dir_if_not_exists(val_real_path)\n",
    "val_mask_path = os.path.join(val_path, 'mask')\n",
    "iu.create_dir_if_not_exists(val_mask_path)\n",
    "#test\n",
    "test_path = os.path.join(training_path, 'test')\n",
    "iu.create_dir_if_not_exists(test_path)\n",
    "test_real_path = os.path.join(test_path, 'real')\n",
    "iu.create_dir_if_not_exists(test_real_path)\n",
    "test_mask_path = os.path.join(test_path, 'mask')\n",
    "iu.create_dir_if_not_exists(test_mask_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e9c463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy training images to training folder\n",
    "i = 0\n",
    "for image in train_images:\n",
    "    shutil.move(image.data_real_path, os.path.join(\n",
    "        train_real_path, os.path.basename(image.data_real_path)))\n",
    "    shutil.move(image.label_path, os.path.join(\n",
    "        train_mask_path, os.path.basename(image.label_path)))\n",
    "    i += 1\n",
    "    iu.print_progress(i, len(train_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e90d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy training images to training folder\n",
    "i = 0\n",
    "for image in val_images:\n",
    "    shutil.move(image.data_real_path, os.path.join(\n",
    "        val_real_path, os.path.basename(image.data_real_path)))\n",
    "    shutil.move(image.label_path, os.path.join(\n",
    "        val_mask_path, os.path.basename(image.label_path)))\n",
    "    i += 1\n",
    "    iu.print_progress(i, len(val_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b4a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_file(file_path):\n",
    "    \"\"\"\n",
    "    Deletes the specified file if it exists.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the file to delete.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If the file does not exist.\n",
    "    - OSError: If there is an error during deletion.\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        # print(f\"Deleted file: {file_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bcf522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, sys\n",
    "\n",
    "# --- helpers -----------------------------------------------------------------\n",
    "def _bytes_feature(value):\n",
    "    if isinstance(value, np.ndarray):\n",
    "        value = value.tobytes()\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    if not isinstance(value, (list, tuple)):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    if isinstance(value, np.ndarray):\n",
    "        value = value.flatten().tolist()\n",
    "    elif isinstance(value, (list, tuple)):\n",
    "        value = list(value)\n",
    "    else:\n",
    "        value = [value]\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def write_tfrecords(image_paths, mask_paths, tfrecord_filename):\n",
    "    \"\"\"\n",
    "    Serialise paired .npz images & masks into a TFRecord, including temporal and location coords.\n",
    "    \"\"\"\n",
    "    assert len(image_paths) == len(mask_paths), \"image_paths and mask_paths length mismatch\"\n",
    "    total = len(image_paths)\n",
    "    #options = tf.io.TFRecordOptions(compression_type=\"GZIP\")   # or \"ZLIB\"\n",
    "    with tf.io.TFRecordWriter(tfrecord_filename) as writer:\n",
    "        for i, (img_p, msk_p) in enumerate(zip(image_paths, mask_paths), 1):\n",
    "            # load image .npz (expects keys: array1, temporal_coords, location_coords)\n",
    "            with np.load(img_p) as img_npz:\n",
    "                img_array = img_npz['array1'].astype(np.float32)\n",
    "                temporal_coords = img_npz['temporal_coords'].astype(np.float32).flatten()   # shape (2,)\n",
    "                location_coords = img_npz['location_coords'].astype(np.float32).flatten()   # shape (2,)\n",
    "\n",
    "            with np.load(msk_p) as msk_npz:\n",
    "                mask_array = msk_npz[msk_npz.files[0]].astype(np.uint8)\n",
    "\n",
    "            # (h, w, c) or (c, h, w)?\n",
    "            if img_array.shape[0] <= 10:  # (bands, H, W)\n",
    "                c, h, w = img_array.shape\n",
    "                img_array = np.transpose(img_array, (1, 2, 0))  # (H, W, C) for TF\n",
    "            else:\n",
    "                h, w, c = img_array.shape\n",
    "\n",
    "            # build Example\n",
    "            example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                \"image_raw\": _bytes_feature(img_array),\n",
    "                \"mask_raw\":  _bytes_feature(mask_array),\n",
    "                \"height\":    _int64_feature(h),\n",
    "                \"width\":     _int64_feature(w),\n",
    "                \"channels\":  _int64_feature(c),\n",
    "                \"image_name\": _bytes_feature(os.path.basename(img_p).encode()),\n",
    "                \"mask_name\":  _bytes_feature(os.path.basename(msk_p).encode()),\n",
    "                \"temporal_coords\": _float_feature(temporal_coords),    # [year, doy]\n",
    "                \"location_coords\": _float_feature(location_coords),    # [lat, lon]\n",
    "            }))\n",
    "            writer.write(example.SerializeToString())\n",
    "\n",
    "            if i % 500 == 0:\n",
    "               writer.flush()\n",
    "            # progress bar\n",
    "            pct = int(i * 100 / total)\n",
    "            sys.stdout.write(f\"\\r▶  {pct:3d}%\")\n",
    "            sys.stdout.flush()\n",
    "            delete_file(img_p)\n",
    "            delete_file(msk_p)\n",
    "\n",
    "    sys.stdout.write(\"\\r✔  100%  TFRecord written → {}\\n\".format(tfrecord_filename))\n",
    "    sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a7886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import image_utils as iu\n",
    "img_paths = sorted(iu.get_all_files(train_real_path, '*.npz', True))\n",
    "mask_paths = sorted(iu.get_all_files(train_mask_path, '*.npz', True))\n",
    "print('real:',len(img_paths), 'mask:', len(mask_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e645af",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tfrecords(img_paths, mask_paths, r\"D:\\ssm_footprint_train.tfrecord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d617af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import image_utils as iu\n",
    "img_paths = sorted(iu.get_all_files(val_real_path, '*.npz', True))\n",
    "mask_paths = sorted(iu.get_all_files(val_mask_path, '*.npz', True))\n",
    "print('real:',len(img_paths), 'mask:', len(mask_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee74f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tfrecords(img_paths, mask_paths, r\"D:\\ssm_footprint_val.tfrecord\") "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
