{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irnJYvm26PTk"
      },
      "source": [
        "# Land Cover Mapping with Prithvi EO Model\n",
        "This notebook demonstrates the workflow for downloading satellite data, preprocessing, running inference with the Prithvi EO segmentation model, and merging results into a single GeoTIFF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/easare377/Prithvi-EO-Segmentation/blob/main/create_landmap_prithvi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UB1P92Gwocex",
        "outputId": "dd08dcd1-42d6-431d-fcc2-e19844973c3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting terratorch\n",
            "  Downloading terratorch-1.0.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting torch==2.7.0 (from terratorch)\n",
            "  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting torchvision==0.22.0 (from terratorch)\n",
            "  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting rioxarray==0.19.0 (from terratorch)\n",
            "  Downloading rioxarray-0.19.0-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting albumentations==1.4.6 (from terratorch)\n",
            "  Downloading albumentations-1.4.6-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting albucore==0.0.16 (from terratorch)\n",
            "  Downloading albucore-0.0.16-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting rasterio==1.4.3 (from terratorch)\n",
            "  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting torchmetrics==1.7.1 (from terratorch)\n",
            "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting geopandas==1.0.1 (from terratorch)\n",
            "  Downloading geopandas-1.0.1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting lightly==1.5.20 (from terratorch)\n",
            "  Downloading lightly-1.5.20-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting jsonargparse<=4.35.0 (from terratorch)\n",
            "  Downloading jsonargparse-4.35.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting h5py==3.13.0 (from terratorch)\n",
            "  Downloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting mlflow==2.22.0 (from terratorch)\n",
            "  Downloading mlflow-2.22.0-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting lightning==2.5.1.post0 (from terratorch)\n",
            "  Downloading lightning-2.5.1.post0-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting segmentation-models-pytorch<=0.4.0 (from terratorch)\n",
            "  Downloading segmentation_models_pytorch-0.4.0-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting pytest==8.3.5 (from terratorch)\n",
            "  Downloading pytest-8.3.5-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting torchgeo==0.7.0 (from terratorch)\n",
            "  Downloading torchgeo-0.7.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: einops==0.8.1 in /usr/local/lib/python3.11/dist-packages (from terratorch) (0.8.1)\n",
            "Requirement already satisfied: timm>=1.0.15 in /usr/local/lib/python3.11/dist-packages (from terratorch) (1.0.19)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from terratorch) (0.34.1)\n",
            "Collecting tifffile==2025.3.30 (from terratorch)\n",
            "  Downloading tifffile-2025.3.30-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.16->terratorch) (2.0.2)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.16->terratorch) (4.12.0.88)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.6->terratorch) (1.16.0)\n",
            "Requirement already satisfied: scikit-image>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.6->terratorch) (0.25.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.6->terratorch) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.6->terratorch) (4.14.1)\n",
            "Requirement already satisfied: scikit-learn>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.6->terratorch) (1.6.1)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.6->terratorch) (2.11.7)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from geopandas==1.0.1->terratorch) (0.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from geopandas==1.0.1->terratorch) (25.0)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from geopandas==1.0.1->terratorch) (2.2.2)\n",
            "Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from geopandas==1.0.1->terratorch) (3.7.1)\n",
            "Requirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from geopandas==1.0.1->terratorch) (2.1.1)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from lightly==1.5.20->terratorch) (2025.7.14)\n",
            "Collecting hydra-core>=1.0.0 (from lightly==1.5.20->terratorch)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting lightly_utils~=0.0.0 (from lightly==1.5.20->terratorch)\n",
            "  Downloading lightly_utils-0.0.2-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: python_dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from lightly==1.5.20->terratorch) (2.9.0.post0)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from lightly==1.5.20->terratorch) (2.32.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from lightly==1.5.20->terratorch) (1.17.0)\n",
            "Requirement already satisfied: tqdm>=4.44 in /usr/local/lib/python3.11/dist-packages (from lightly==1.5.20->terratorch) (4.67.1)\n",
            "Collecting pytorch_lightning>=1.0.4 (from lightly==1.5.20->terratorch)\n",
            "  Downloading pytorch_lightning-2.5.2-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.11/dist-packages (from lightly==1.5.20->terratorch) (2.5.0)\n",
            "Collecting aenum>=3.1.11 (from lightly==1.5.20->terratorch)\n",
            "  Downloading aenum-3.1.16-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning==2.5.1.post0->terratorch) (2025.3.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning==2.5.1.post0->terratorch)\n",
            "  Downloading lightning_utilities-0.15.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting packaging (from geopandas==1.0.1->terratorch)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting mlflow-skinny==2.22.0 (from mlflow==2.22.0->terratorch)\n",
            "  Downloading mlflow_skinny-2.22.0-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.22.0->terratorch) (3.1.1)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.22.0->terratorch) (3.1.6)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow==2.22.0->terratorch)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow==2.22.0->terratorch)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow==2.22.0->terratorch)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow==2.22.0->terratorch)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.22.0->terratorch) (3.8.2)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.22.0->terratorch) (3.10.0)\n",
            "Requirement already satisfied: pyarrow<20,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.22.0->terratorch) (18.1.0)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==2.22.0->terratorch) (2.0.41)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest==8.3.5->terratorch) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest==8.3.5->terratorch) (1.6.0)\n",
            "Collecting affine (from rasterio==1.4.3->terratorch)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio==1.4.3->terratorch) (25.3.0)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio==1.4.3->terratorch) (8.2.1)\n",
            "Collecting cligj>=0.5 (from rasterio==1.4.3->terratorch)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting click-plugins (from rasterio==1.4.3->terratorch)\n",
            "  Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio==1.4.3->terratorch) (3.2.3)\n",
            "Requirement already satisfied: xarray>=2024.7.0 in /usr/local/lib/python3.11/dist-packages (from rioxarray==0.19.0->terratorch) (2025.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->terratorch) (3.18.0)\n",
            "Collecting sympy>=1.13.3 (from torch==2.7.0->terratorch)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->terratorch) (3.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.0->terratorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.0->terratorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.0->terratorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.0->terratorch)\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.0->terratorch)\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.0->terratorch)\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.0->terratorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.0->terratorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.0->terratorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.0->terratorch)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.0->terratorch)\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.0->terratorch)\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.0->terratorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.0->terratorch)\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.3.0 (from torch==2.7.0->terratorch)\n",
            "  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting fiona>=1.8.22 (from torchgeo==0.7.0->terratorch)\n",
            "  Downloading fiona-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kornia>=0.7.4 (from torchgeo==0.7.0->terratorch)\n",
            "  Downloading kornia-0.8.1-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: pillow>=9.2 in /usr/local/lib/python3.11/dist-packages (from torchgeo==0.7.0->terratorch) (11.3.0)\n",
            "Collecting rtree>=1.0.1 (from torchgeo==0.7.0->terratorch)\n",
            "  Downloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow==2.22.0->terratorch) (5.5.2)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow==2.22.0->terratorch) (3.1.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.22.0->mlflow==2.22.0->terratorch)\n",
            "  Downloading databricks_sdk-0.61.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow==2.22.0->terratorch) (0.116.1)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow==2.22.0->terratorch) (3.1.45)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow==2.22.0->terratorch) (8.7.0)\n",
            "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==2.22.0->mlflow==2.22.0->terratorch)\n",
            "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==2.22.0->mlflow==2.22.0->terratorch)\n",
            "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow==2.22.0->terratorch) (5.29.5)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow==2.22.0->terratorch) (0.5.3)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow==2.22.0->terratorch) (0.35.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch==2.7.0->terratorch) (75.2.0)\n",
            "Collecting efficientnet-pytorch>=0.6.1 (from segmentation-models-pytorch<=0.4.0->terratorch)\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pretrainedmodels>=0.7.1 (from segmentation-models-pytorch<=0.4.0->terratorch)\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->terratorch) (1.1.5)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm>=1.0.15->terratorch) (0.5.3)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow==2.22.0->terratorch) (1.1.3)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow==2.22.0->terratorch) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow==2.22.0->terratorch) (2.2.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow==2.22.0->terratorch) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow==2.22.0->terratorch) (3.1.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning==2.5.1.post0->terratorch) (3.12.14)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow==2.22.0->terratorch)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow==2.22.0->terratorch)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.0.0->lightly==1.5.20->terratorch) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.0.0->lightly==1.5.20->terratorch) (4.9.3)\n",
            "Collecting kornia_rs>=0.1.9 (from kornia>=0.7.4->torchgeo==0.7.0->terratorch)\n",
            "  Downloading kornia_rs-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: rich<14.0,>=12.3.0 in /usr/local/lib/python3.11/dist-packages (from lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo==0.7.0->terratorch) (13.9.4)\n",
            "Collecting tensorboardX<3.0,>=2.2 (from lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo==0.7.0->terratorch)\n",
            "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting bitsandbytes<1.0,>=0.45.2 (from lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo==0.7.0->terratorch)\n",
            "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.22.0->terratorch) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.22.0->terratorch) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.22.0->terratorch) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow==2.22.0->terratorch) (1.4.8)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->geopandas==1.0.1->terratorch) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->geopandas==1.0.1->terratorch) (2025.2)\n",
            "Collecting munch (from pretrainedmodels>=0.7.1->segmentation-models-pytorch<=0.4.0->terratorch)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.0->albumentations==1.4.6->terratorch) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.0->albumentations==1.4.6->terratorch) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.0->albumentations==1.4.6->terratorch) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->lightly==1.5.20->terratorch) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->lightly==1.5.20->terratorch) (3.10)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.6->terratorch) (2.37.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.21.0->albumentations==1.4.6->terratorch) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.2->albumentations==1.4.6->terratorch) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.2->albumentations==1.4.6->terratorch) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.22.0->terratorch) (3.2.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.7.0->terratorch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning==2.5.1.post0->terratorch) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning==2.5.1.post0->terratorch) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning==2.5.1.post0->terratorch) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning==2.5.1.post0->terratorch) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning==2.5.1.post0->terratorch) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning==2.5.1.post0->terratorch) (1.20.1)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow==2.22.0->terratorch) (2.38.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==2.22.0->mlflow==2.22.0->terratorch) (0.47.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.22.0->mlflow==2.22.0->terratorch) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.22.0->mlflow==2.22.0->terratorch) (3.23.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo==0.7.0->terratorch) (0.17.0)\n",
            "Collecting typeshed-client>=2.1.0 (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo==0.7.0->terratorch)\n",
            "  Downloading typeshed_client-2.8.2-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow==2.22.0->terratorch)\n",
            "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0,>=12.3.0->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo==0.7.0->terratorch) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0,>=12.3.0->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo==0.7.0->terratorch) (2.19.2)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny==2.22.0->mlflow==2.22.0->terratorch) (0.16.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.22.0->mlflow==2.22.0->terratorch) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow==2.22.0->terratorch) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow==2.22.0->terratorch) (4.9.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0,>=12.3.0->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo==0.7.0->terratorch) (0.1.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==2.22.0->mlflow==2.22.0->terratorch) (4.9.0)\n",
            "Requirement already satisfied: importlib_resources>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from typeshed-client>=2.1.0->jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo==0.7.0->terratorch) (6.5.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==2.22.0->mlflow==2.22.0->terratorch) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow==2.22.0->terratorch) (0.6.1)\n",
            "Downloading terratorch-1.0.2-py3-none-any.whl (486 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.6/486.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading albucore-0.0.16-py3-none-any.whl (9.5 kB)\n",
            "Downloading albumentations-1.4.6-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading geopandas-1.0.1-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.6/323.6 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonargparse-4.35.0-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.0/211.0 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightly-1.5.20-py3-none-any.whl (851 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m851.6/851.6 kB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.5.1.post0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.0/819.0 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow-2.22.0-py3-none-any.whl (29.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.0/29.0 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest-8.3.5-py3-none-any.whl (343 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.6/343.6 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rioxarray-0.19.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tifffile-2025.3.30-py3-none-any.whl (226 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchgeo-0.7.0-py3-none-any.whl (604 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m605.0/605.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-2.22.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading segmentation_models_pytorch-0.4.0-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aenum-3.1.16-py3-none-any.whl (165 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.6/165.6 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fiona-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click_plugins-1.1.1.2-py2.py3-none-any.whl (11 kB)\n",
            "Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia-0.8.1-py2.py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightly_utils-0.0.2-py3-none-any.whl (6.4 kB)\n",
            "Downloading lightning_utilities-0.15.1-py3-none-any.whl (29 kB)\n",
            "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.2-py3-none-any.whl (825 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (541 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.1/541.1 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.61.0-py3-none-any.whl (680 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m680.6/680.6 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading kornia_rs-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Downloading typeshed_client-2.8.2-py3-none-any.whl (760 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m760.5/760.5 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16426 sha256=f78198020bc222873c979ec6ea79165cfde26de0d8542e4d6bb59a3ad59958bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/6f/9b/231a832f811ab6ebb1b32455b177ffc6b8b1cd8de19de70c09\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=b8f8b9b28f8dc4d93c5d9d3986193cc8679f573747162611ec078b405ad065dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/5b/96/fd94bc35962d7c6b699e8814db545155ac91d2b95785e1b035\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: nvidia-cusparselt-cu12, aenum, typeshed-client, triton, tifffile, sympy, rtree, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, munch, lightly_utils, kornia_rs, jsonargparse, h5py, graphql-core, cligj, click-plugins, affine, tensorboardX, rasterio, pytest, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, lightning-utilities, hydra-core, gunicorn, graphql-relay, fiona, docker, alembic, albucore, opentelemetry-semantic-conventions, nvidia-cusolver-cu12, graphene, geopandas, databricks-sdk, torch, rioxarray, opentelemetry-sdk, albumentations, torchvision, torchmetrics, mlflow-skinny, kornia, efficientnet-pytorch, bitsandbytes, pytorch_lightning, pretrainedmodels, mlflow, segmentation-models-pytorch, lightning, lightly, torchgeo, terratorch\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: tifffile\n",
            "    Found existing installation: tifffile 2025.6.11\n",
            "    Uninstalling tifffile-2025.6.11:\n",
            "      Successfully uninstalled tifffile-2025.6.11\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.14.0\n",
            "    Uninstalling h5py-3.14.0:\n",
            "      Successfully uninstalled h5py-3.14.0\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 8.4.1\n",
            "    Uninstalling pytest-8.4.1:\n",
            "      Successfully uninstalled pytest-8.4.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: albucore\n",
            "    Found existing installation: albucore 0.0.24\n",
            "    Uninstalling albucore-0.0.24:\n",
            "      Successfully uninstalled albucore-0.0.24\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: geopandas\n",
            "    Found existing installation: geopandas 1.1.1\n",
            "    Uninstalling geopandas-1.1.1:\n",
            "      Successfully uninstalled geopandas-1.1.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 2.0.8\n",
            "    Uninstalling albumentations-2.0.8:\n",
            "      Successfully uninstalled albumentations-2.0.8\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aenum-3.1.16 affine-2.4.0 albucore-0.0.16 albumentations-1.4.6 alembic-1.16.4 bitsandbytes-0.46.1 click-plugins-1.1.1.2 cligj-0.7.2 databricks-sdk-0.61.0 docker-7.1.0 efficientnet-pytorch-0.7.1 fiona-1.10.1 geopandas-1.0.1 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 h5py-3.13.0 hydra-core-1.3.2 jsonargparse-4.35.0 kornia-0.8.1 kornia_rs-0.1.9 lightly-1.5.20 lightly_utils-0.0.2 lightning-2.5.1.post0 lightning-utilities-0.15.1 mlflow-2.22.0 mlflow-skinny-2.22.0 munch-4.0.0 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 opentelemetry-api-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 packaging-24.2 pretrainedmodels-0.7.4 pytest-8.3.5 pytorch_lightning-2.5.2 rasterio-1.4.3 rioxarray-0.19.0 rtree-1.4.0 segmentation-models-pytorch-0.4.0 sympy-1.14.0 tensorboardX-2.6.4 terratorch-1.0.2 tifffile-2025.3.30 torch-2.7.0 torchgeo-0.7.0 torchmetrics-1.7.1 torchvision-0.22.0 triton-3.3.0 typeshed-client-2.8.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "packaging"
                ]
              },
              "id": "aeff4f666fc048a99fe5b8bd67d01fb3"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install terratorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJyJ7Y636PTp"
      },
      "source": [
        "# Install Dependencies and Mount Google Drive\n",
        "Install required packages and mount Google Drive for data access and storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aMQ4quluocex"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from osgeo import gdal\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMRe7x31qDUP",
        "outputId": "24e21f4a-a3d5-461d-b57d-49f62f883b55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AoVecRdgocex"
      },
      "outputs": [],
      "source": [
        "\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import pathlib\n",
        "\n",
        "\n",
        "def download_file(url, dest_path, chunk_size=1024*1024):\n",
        "    \"\"\"\n",
        "    Download a large file from a URL with a progress bar.\n",
        "    Args:\n",
        "        url (str): File URL.\n",
        "        dest_path (str): Destination file path.\n",
        "        chunk_size (int): Download chunk size in bytes.\n",
        "    \"\"\"\n",
        "    response = requests.get(url, stream=True)\n",
        "    total = int(response.headers.get('content-length', 0))\n",
        "    with open(dest_path, 'wb') as file, tqdm(\n",
        "        desc=f\"Downloading {dest_path}\",\n",
        "        total=total,\n",
        "        unit='B',\n",
        "        unit_scale=True,\n",
        "        unit_divisor=1024,\n",
        "    ) as bar:\n",
        "        for data in response.iter_content(chunk_size=chunk_size):\n",
        "            size = file.write(data)\n",
        "            bar.update(size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OBv7G466PTr"
      },
      "source": [
        "# Download and Prepare Satellite Data\n",
        "Download large satellite images and prepare them for processing and inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6-dZbtouocey"
      },
      "outputs": [],
      "source": [
        "def zero_pad_array(input_array, new_shape):\n",
        "    \"\"\"\n",
        "    Zero-pad the input_array to the specified new_shape.\n",
        "    Args:\n",
        "        input_array (numpy.ndarray): Input array of shape (height, width, ...).\n",
        "        new_shape (tuple): Desired new shape (new_height, new_width, ...).\n",
        "    Returns:\n",
        "        numpy.ndarray: Zero-padded array of shape (new_height, new_width, ...).\n",
        "    \"\"\"\n",
        "    h, w = input_array.shape[:2]\n",
        "    new_h, new_w = new_shape[:2]\n",
        "    pad_h = max(new_h - h, 0)\n",
        "    pad_w = max(new_w - w, 0)\n",
        "    pad_values = [(0, pad_h), (0, pad_w)]\n",
        "    pad_values += [(0, 0)] * (input_array.ndim - 2)\n",
        "    return np.pad(input_array, pad_values, mode='constant', constant_values=0)\n",
        "\n",
        "def crop_np_array(arr, left, top, right, bottom):\n",
        "    \"\"\"\n",
        "    Crop the input NumPy array to the specified bounding box.\n",
        "\n",
        "    Args:\n",
        "        arr (numpy.ndarray): Input array to be cropped.\n",
        "        left (int): Left coordinate of the bounding box.\n",
        "        top (int): Top coordinate of the bounding box.\n",
        "        right (int): Right coordinate of the bounding box.\n",
        "        bottom (int): Bottom coordinate of the bounding box.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Cropped array within the specified bounding box.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the bounding box coordinates are invalid or exceed the array size.\n",
        "    \"\"\"\n",
        "    # Validate the bounding box coordinates\n",
        "    if left < 0 or top < 0 or right <= left or bottom <= top:\n",
        "        raise ValueError(\"Invalid bounding box coordinates.\")\n",
        "\n",
        "    # Get the dimensions of the input array\n",
        "    arr_height, arr_width = arr.shape[:2]\n",
        "\n",
        "    # Check if the bounding box exceeds the array size\n",
        "    if right > arr_width or bottom > arr_height:\n",
        "        raise ValueError(\"Bounding box exceeds the array size.\")\n",
        "\n",
        "    # Crop the array to the specified bounding box\n",
        "    cropped_arr = arr[top:bottom, left:right]\n",
        "\n",
        "    return cropped_arr\n",
        "\n",
        "def write_geoTiff_bands(output_raster, np_array, position):\n",
        "    \"\"\"\n",
        "    Write a numpy array to a GeoTIFF file.\n",
        "\n",
        "    Parameters:\n",
        "    - output_raster (gdal.Dataset): Output GeoTIFF dataset.\n",
        "    - np_array (numpy.ndarray): Numpy array to be written.\n",
        "    - position (tuple): Top-left position (left, top) to write the array in the GeoTIFF.\n",
        "    \"\"\"\n",
        "    array = np_array\n",
        "    left, top = position\n",
        "    height, width = array.shape[0], array.shape[1]\n",
        "    bands = array.shape[2]\n",
        "    for x in range(bands):\n",
        "        output_raster.GetRasterBand(x + 1).WriteArray(\n",
        "            array[:, :, x].reshape((height, width)), xoff=left, yoff=top)   # Writes my array to the raster\n",
        "    output_raster.FlushCache()\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(path):\n",
        "    if os.path.exists(path):\n",
        "        return\n",
        "    os.makedirs(path)\n",
        "\n",
        "\n",
        "def create_new_geoTiff(dest_path, size, band_count, projection, geo_transform, dtype=gdal.GDT_Byte, compression='DEFLATE', zlevel=9):\n",
        "    \"\"\"\n",
        "    Create a new GeoTIFF file with the specified dimensions, bands, projection, and geotransformation.\n",
        "\n",
        "    Parameters:\n",
        "        - dest_path (str): The path where the new GeoTIFF file will be created.\n",
        "        - size (tuple): A tuple specifying the width and height of the new GeoTIFF in pixels. (width, height)\n",
        "        - band_count (int): The number of bands in the new GeoTIFF.\n",
        "        - projection (str): The projection of the GeoTIFF file in Well-Known Text (WKT) format.\n",
        "        - geo_transform (tuple): A tuple representing the geotransformation parameters for the GeoTIFF.\n",
        "                               (originX, pixelWidth, 0, originY, 0, pixelHeight)\n",
        "        - dtype (int, optional): The data type of the pixel values in the new GeoTIFF. Default is gdal.GDT_Byte (8-bit).\n",
        "        - compression (str, optional): Compression method for the GeoTIFF. Default is 'DEFLATE'.\n",
        "        - zlevel (int, optional): Compression level for the GeoTIFF. Default is 5.\n",
        "    Returns:\n",
        "        gdal.Dataset: The GDAL dataset representing the newly created GeoTIFF file.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the GeoTIFF file with the specified dimensions\n",
        "    width, height = size\n",
        "    driver = gdal.GetDriverByName(\"GTiff\")\n",
        "    # Set compression options\n",
        "    options = ['COMPRESS=' + compression, 'ZLEVEL=' + str(zlevel)]\n",
        "    ds = driver.Create(dest_path, width, height,\n",
        "                       band_count, dtype, options=options)\n",
        "    # Define the projection of the file\n",
        "    ds.SetProjection(projection)\n",
        "    # Specify its coordinates\n",
        "    ds.SetGeoTransform(geo_transform)\n",
        "    return ds\n",
        "\n",
        "def get_geoTiff_extent(geoTiff, image_bounds=None):\n",
        "    \"\"\"\n",
        "    Get the geographic extent (bounding box) of a GeoTIFF.\n",
        "\n",
        "    Parameters:\n",
        "    - geoTiff (GDAL Dataset): Input GeoTIFF dataset object.\n",
        "    - image_bounds (tuple, optional): Bounds of the specific area of interest within the GeoTIFF.\n",
        "                                      Format: (xmin, xmax, ymin, ymax). Default is None,\n",
        "                                      which represents the entire extent of the GeoTIFF.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: Geographic extent (left, right, top, bottom) in the coordinate reference system of the GeoTIFF.\n",
        "\n",
        "    Note:\n",
        "    - The function assumes the input GeoTIFF is in a projected coordinate reference system (CRS).\n",
        "    - If image_bounds is provided, the extent will be calculated based on the specified bounds.\n",
        "      Otherwise, the entire extent of the GeoTIFF will be used.\n",
        "    - The returned extent represents the geographic coordinates (left, right, top, bottom)\n",
        "      within the CRS of the GeoTIFF.\n",
        "\n",
        "    Exceptions:\n",
        "    - ValueError: Raised if the image bounds are outside the valid range of the GeoTIFF.\n",
        "\n",
        "    \"\"\"\n",
        "    xmin_i, xres_i, xskew_i, ymin_i, yskew_r, yres_i = geoTiff.GetGeoTransform()\n",
        "    width, height = geoTiff.RasterXSize, geoTiff.RasterYSize\n",
        "    if image_bounds is None:\n",
        "        xmin, xmax, ymin, ymax = 0, geoTiff.RasterXSize, 0, geoTiff.RasterYSize\n",
        "    else:\n",
        "        xmin, xmax, ymin, ymax = image_bounds\n",
        "        if xmin < 0 or xmax > width or ymin < 0 or ymax > height:\n",
        "            raise ValueError(\n",
        "                \"Image bounds are outside the valid range of the GeoTIFF.\")\n",
        "    left = xmin_i + (xmin * xres_i)\n",
        "    right = xmin_i + (xmax * xres_i)\n",
        "    top = ymin_i + (ymin * yres_i)\n",
        "    bottom = ymin_i + (ymax * yres_i)\n",
        "    return (left, right, top, bottom)\n",
        "\n",
        "def get_geoTiff_datatype(geoTiff):\n",
        "    \"\"\"\n",
        "    Get the GDAL data type of a GeoTIFF dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - geoTiff (gdal.Dataset): Input GeoTIFF dataset.\n",
        "\n",
        "    Returns:\n",
        "    - int: GDAL data type of the GeoTIFF dataset.\n",
        "\n",
        "    \"\"\"\n",
        "    band = geoTiff.GetRasterBand(1)\n",
        "    return band.DataType\n",
        "\n",
        "def get_geoTiff_numpy_datatype(geoTiff):\n",
        "    \"\"\"\n",
        "    Get the NumPy data type string of a GeoTIFF dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - geoTiff (gdal.Dataset): Input GeoTIFF dataset.\n",
        "\n",
        "    Returns:\n",
        "    - str: NumPy data type string of the GeoTIFF dataset.\n",
        "\n",
        "    \"\"\"\n",
        "    gt_dtype = get_geoTiff_datatype(geoTiff)\n",
        "    gdal_to_numpy_datatype = {\n",
        "        gdal.GDT_Byte: 'uint8',\n",
        "        gdal.GDT_UInt16: 'uint16',\n",
        "        gdal.GDT_Int16: 'int16',\n",
        "        gdal.GDT_UInt32: 'uint32',\n",
        "        gdal.GDT_Int32: 'int32',\n",
        "        gdal.GDT_Float32: 'float32',\n",
        "        gdal.GDT_Float64: 'float64'\n",
        "    }\n",
        "    numpy_datatype = gdal_to_numpy_datatype.get(gt_dtype, None)\n",
        "    return numpy_datatype\n",
        "\n",
        "\n",
        "def crop_geoTiff(geoTiff, left, top, right, bottom, dtype=None):\n",
        "    \"\"\"\n",
        "    Crop a GeoTIFF array to the specified region.\n",
        "\n",
        "    Parameters:\n",
        "    - geoTiff (gdal.Dataset): Input GeoTIFF dataset.\n",
        "    - left (int): Left coordinate of the crop region.\n",
        "    - top (int): Top coordinate of the crop region.\n",
        "    - right (int): Right coordinate of the crop region.\n",
        "    - bottom (int): Bottom coordinate of the crop region.\n",
        "    - dtype (str or None, optional): Desired data type of the output array.\n",
        "    If set to None, the data type is inferred from the band. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "    - output (numpy.ndarray): Cropped array with dimensions (height, width, bands).\n",
        "\n",
        "    Raises:\n",
        "    - ValueError: If the crop dimensions exceed the size of the GeoTIFF.\n",
        "    \"\"\"\n",
        "    if (int(right) > geoTiff.RasterXSize) or (int(bottom) > geoTiff.RasterYSize):\n",
        "        # print(right, bottom)\n",
        "        # print(geoTiff.RasterXSize, geoTiff.RasterYSize)\n",
        "        raise ValueError('Crop dimensions exceed the size of the GeoTIFF.')\n",
        "    if dtype is None:\n",
        "        dtype = get_geoTiff_numpy_datatype(geoTiff)\n",
        "    width = abs(right - left)\n",
        "    height = abs(top - bottom)\n",
        "    output = np.zeros(\n",
        "        (int(height), int(width), geoTiff.RasterCount), dtype)\n",
        "    # bands = [None] * geoTiff.RasterCount\n",
        "    for x in range(geoTiff.RasterCount):\n",
        "        band = geoTiff.GetRasterBand(x + 1).ReadAsArray(left, top,\n",
        "                                                        int(width), int(height))\n",
        "        output[..., x] = band\n",
        "    return output\n",
        "\n",
        "def get_geoTiff_part(geoTiff, left, top, right, bottom, dtype='uint16'):\n",
        "    c_gt = crop_geoTiff(geoTiff, left, top, right, bottom, dtype)\n",
        "    extent = get_geoTiff_extent(geoTiff, (left, right, top, bottom))\n",
        "    return c_gt, extent\n",
        "\n",
        "def crop_geoTiff_into_grids(geoTiff, max_grid_shape, yield_results=False, dtype = 'uint16'):\n",
        "    \"\"\"\n",
        "    Split a GeoTIFF into grids of numpy arrays with a specified maximum shape.\n",
        "\n",
        "    Parameters:\n",
        "    - geoTiff (gdal.Dataset): Input GeoTIFF dataset.\n",
        "    - max_grid_shape (tuple): Maximum shape of each grid (rows, columns).\n",
        "    - yield_results (bool, optional): Whether to yield each grid individually or return all grids in a list.\n",
        "                                      Defaults to True (yield results).\n",
        "\n",
        "    Yields or Returns:\n",
        "    - grid (numpy.ndarray): Numpy array representing a divided grid.\n",
        "      (Yielded if yield_results=True, Returned as a list if yield_results=False)\n",
        "\n",
        "    Raises:\n",
        "    - ValueError: If the max_grid_shape is invalid or exceeds the size of the GeoTIFF.\n",
        "    \"\"\"\n",
        "    if not isinstance(max_grid_shape, tuple) or len(max_grid_shape) != 2 or \\\n",
        "       max_grid_shape[0] <= 0 or max_grid_shape[1] <= 0:\n",
        "        raise ValueError(\n",
        "            \"Invalid max_grid_shape. It should be a tuple of two positive integers.\")\n",
        "\n",
        "    if max_grid_shape[0] > geoTiff.RasterYSize or max_grid_shape[1] > geoTiff.RasterXSize:\n",
        "        raise ValueError(\"max_grid_shape exceeds the size of the GeoTIFF.\")\n",
        "\n",
        "    rows, cols = geoTiff.RasterYSize, geoTiff.RasterXSize\n",
        "    max_grid_height, max_grid_width = max_grid_shape\n",
        "    total_grids = np.ceil(rows / max_grid_height) * np.ceil(cols / max_grid_width)\n",
        "    if yield_results:\n",
        "        progress = 0\n",
        "        for r in range(0, rows, max_grid_height):\n",
        "            for c in range(0, cols, max_grid_width):\n",
        "                left, top = c, r\n",
        "                right, bottom = min(\n",
        "                    c + max_grid_width, cols), min(r + max_grid_height, rows)\n",
        "\n",
        "                grid, extent = get_geoTiff_part(\n",
        "                    geoTiff, left, top, right, bottom, dtype)\n",
        "                progress += 1\n",
        "                yield (grid, extent, (left, top, right, bottom), (progress, total_grids))\n",
        "    else:\n",
        "        grids = []\n",
        "        for r in range(0, rows, max_grid_height):\n",
        "            for c in range(0, cols, max_grid_width):\n",
        "                left, top = c, r\n",
        "                right, bottom = min(\n",
        "                    c + max_grid_width, cols), min(r + max_grid_height, rows)\n",
        "\n",
        "                grid, extent = get_geoTiff_part(\n",
        "                    geoTiff, left, top, right, bottom)\n",
        "                grids.append((grid, extent, (left, top, right, bottom)))\n",
        "        return grids\n",
        "\n",
        "def crop_and_process_geoTiff(geoTiff, process_func, max_grid_shape=None, dtype = 'uint16'):\n",
        "    \"\"\"\n",
        "    Split a GeoTIFF into grids, process each grid, and write the processed grids to an output GeoTIFF.\n",
        "\n",
        "    Parameters:\n",
        "    - dataset (gdal.Dataset): Input GeoTIFF dataset.\n",
        "    - max_grid_shape (tuple): Maximum shape of each grid (rows, columns).\n",
        "    - output_tiff (str): Output GeoTIFF file path.\n",
        "    Raises:\n",
        "    - ValueError: If the max_grid_shape is invalid or exceeds the size of the GeoTIFF.\n",
        "    \"\"\"\n",
        "    if max_grid_shape is None:\n",
        "        max_grid_shape = geoTiff.RasterYSize, geoTiff.RasterXSize\n",
        "    if not isinstance(max_grid_shape, tuple) or len(max_grid_shape) != 2 or \\\n",
        "       max_grid_shape[0] <= 0 or max_grid_shape[1] <= 0:\n",
        "        raise ValueError(\n",
        "            \"Invalid max_grid_shape. It should be a tuple of two positive integers.\")\n",
        "\n",
        "    if max_grid_shape[0] > geoTiff.RasterYSize or max_grid_shape[1] > geoTiff.RasterXSize:\n",
        "        raise ValueError(\"max_grid_shape exceeds the size of the GeoTIFF.\")\n",
        "\n",
        "    # Iterate over each grid individually\n",
        "    for i, grid in enumerate(crop_geoTiff_into_grids(geoTiff, max_grid_shape, True, dtype)):\n",
        "        # Process each grid (Replace with your processing code)\n",
        "        np_gt_part, extent, image_coord, progress = grid\n",
        "        processed_grid = process_func(np_gt_part, extent, image_coord, progress)\n",
        "\n",
        "def get_file_or_foldername(path):\n",
        "    return os.path.basename(path)\n",
        "\n",
        "def real_image_preprocessing_func(np_gt):\n",
        "    \"\"\"\n",
        "    1) Clip the raw array to [0, 6000].\n",
        "    2) Normalize each band using the provided mean and std (computed from raw data).\n",
        "    3) Clip the result to [0, 1].\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure float32\n",
        "    np_gt = np_gt.astype(np.float32)\n",
        "\n",
        "    # Provided normalization statistics for raw values (0..6000 range):\n",
        "    data_mean = np.array([1635.8452, 1584.4594, 1456.8425, 2926.6663, 2135.001, 1352.7313], dtype=np.float32)\n",
        "\n",
        "    data_std = np.array([884.3994, 815.4016, 839.0293, 1055.6382, 751.4628, 628.5323], dtype=np.float32)\n",
        "\n",
        "\n",
        "    # 1) Clip the raw values to [0..6000]\n",
        "    #np_gt = np.clip(np_gt, 0, 6000)\n",
        "\n",
        "    # 2) Bandwise normalization: (value - mean) / std\n",
        "    #    Assuming np_gt has shape (H, W, 6), broadcasting will apply along the last dimension\n",
        "    np_gt = (np_gt - data_mean) / data_std\n",
        "\n",
        "    # 3) Finally, clip to [0..1] if desired\n",
        "    #np_gt = np.clip(np_gt, 0, 1)\n",
        "\n",
        "    return np_gt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Xh_4cU6PTs"
      },
      "source": [
        "# Preprocessing and Utility Functions\n",
        "Define helper functions for padding, cropping, writing GeoTIFFs, and preprocessing images for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0wCpS6csIj_",
        "outputId": "6788383a-001c-4e66-d313-f39371a5c26c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading /content/drive/MyDrive/SCO_training/sentinel_images/2024.tif: 100%|██████████| 12.9G/12.9G [14:37<00:00, 15.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "dest_path = '/content/drive/MyDrive/SCO_training/sentinel_images'\n",
        "geotiff_image_path = os.path.join(dest_path, '2024.tif')\n",
        "create_dir_if_not_exists(dest_path)\n",
        "download_file('https://sco-training.s3.us-east-2.amazonaws.com/2024.tif', geotiff_image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNJlD6vusOKP",
        "outputId": "a721d0eb-6654-42dc-cde3-41d772c691fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PoQehs9Qocey"
      },
      "outputs": [],
      "source": [
        "def predict_mask(unet, np_img, input_shape):\n",
        "    # Zero-pad if needed\n",
        "    height, width = np_img.shape[:2]\n",
        "    target_height, target_width = input_shape\n",
        "    # Preprocess\n",
        "    np_img = real_image_preprocessing_func(np_img)\n",
        "    if (height != target_height) or (width != target_width):\n",
        "        np_img = zero_pad_array(np_img, (target_height, target_width))\n",
        "    # Convert to torch tensor [B, C, H, W]\n",
        "    img_tensor = torch.from_numpy(np_img).permute(2, 0, 1).unsqueeze(0)\n",
        "    img_tensor = img_tensor.float().to(device)\n",
        "    # Inference\n",
        "    unet.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = unet(img_tensor).output  # [1, n_classes, H, W]\n",
        "    # Argmax -> [H, W]\n",
        "    pred_mask = logits.argmax(dim=1).squeeze(0).cpu().numpy()\n",
        "    # Crop back if zero-padded\n",
        "    pred_mask = crop_np_array(pred_mask, 0, 0, width, height)\n",
        "    #print(pred_mask.shape)\n",
        "    pred_mask = pred_mask.reshape((pred_mask.shape[0], pred_mask.shape[1], 1))\n",
        "    return pred_mask\n",
        "\n",
        "\n",
        "def pre_process_func(dest_gt, model, max_grid_shape):\n",
        "    def predict_grid(np_gt, extent, image_coord, progress):\n",
        "        mask_np_gt = predict_mask(model, np_gt, max_grid_shape)\n",
        "        position = image_coord[0], image_coord[1]\n",
        "        #print(mask_np_gt.shape)\n",
        "        write_geoTiff_bands(dest_gt, mask_np_gt, position)\n",
        "        # Optionally log progress\n",
        "    return predict_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ioTJfkt3ocez"
      },
      "outputs": [],
      "source": [
        "# Usage example\n",
        "def run_inference(gt_paths, prediction_path, mc_unet):\n",
        "    # Move model to GPU (if available) once\n",
        "    mc_unet.to(device)\n",
        "    mc_unet.eval()\n",
        "\n",
        "    dest_path = prediction_path\n",
        "    create_dir_if_not_exists(dest_path)\n",
        "\n",
        "    for gt_path in gt_paths:\n",
        "        dest_file = os.path.join(dest_path, get_file_or_foldername(gt_path))\n",
        "        if os.path.exists(dest_file):\n",
        "            print(\"skipping\", dest_file)\n",
        "            continue\n",
        "\n",
        "        print(\"Processing\", gt_path)\n",
        "        gt = gdal.Open(gt_path)\n",
        "        proj = gt.GetProjection()\n",
        "        size = gt.RasterXSize, gt.RasterYSize\n",
        "        geo_transform = gt.GetGeoTransform()\n",
        "\n",
        "        dest_gt = create_new_geoTiff(dest_file, size, 1, proj, geo_transform, compression=\"None\")\n",
        "        tile_callback = pre_process_func(dest_gt, mc_unet, (224, 224))\n",
        "\n",
        "        crop_and_process_geoTiff(gt, tile_callback, (224, 224), 'uint16')\n",
        "        gt = None\n",
        "        dest_gt = None\n",
        "        print(dest_file, \"saved\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zExvFPZJ6PTu"
      },
      "source": [
        "# Run Inference and Save Predictions\n",
        "Run the Prithvi EO segmentation model on satellite images and save predicted masks as GeoTIFFs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldXXdMjXocez",
        "outputId": "586e3ef7-fd5e-468b-db83-041e254318a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Decoder FCNDecoder does not have an `includes_head` attribute. Falling back to the value of the registry.\n"
          ]
        }
      ],
      "source": [
        "from terratorch.tasks import SemanticSegmentationTask\n",
        "\n",
        "model = SemanticSegmentationTask(\n",
        "    model_factory=\"EncoderDecoderFactory\",\n",
        "    model_args=dict(\n",
        "        backbone=\"terratorch_prithvi_eo_v2_300_tl\",\n",
        "        backbone_pretrained=False,\n",
        "        backbone_img_size=224,\n",
        "        backbone_bands=[\"BLUE\",\"GREEN\",\"RED\",\"NIR_NARROW\",\"SWIR_1\",\"SWIR_2\"],\n",
        "        necks=[{\"name\":\"SelectIndices\", \"indices\":[1, 5,11,17,23]},\n",
        "               {\"name\":\"ReshapeTokensToImage\"}],\n",
        "        decoder=\"FCNDecoder\",\n",
        "        decoder_channels=256,\n",
        "        num_classes=3,\n",
        "        head_dropout=0.1,\n",
        "    ),\n",
        "    freeze_backbone=False,\n",
        "    freeze_decoder=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAPIJBvCocez",
        "outputId": "4a8e447d-5681-4f1d-ee00-f10384c5874b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SemanticSegmentationTask(\n",
              "  (model): PixelWiseModel(\n",
              "    (encoder): PrithviViT(\n",
              "      (patch_embed): PatchEmbed(\n",
              "        (proj): Conv3d(6, 1024, kernel_size=(1, 16, 16), stride=(1, 16, 16))\n",
              "        (norm): Identity()\n",
              "      )\n",
              "      (temporal_embed_enc): TemporalEncoder()\n",
              "      (location_embed_enc): LocationEncoder()\n",
              "      (blocks): ModuleList(\n",
              "        (0-23): 24 x Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (q_norm): Identity()\n",
              "            (k_norm): Identity()\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls1): Identity()\n",
              "          (drop_path1): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ls2): Identity()\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): FCNDecoder(\n",
              "      (convs): Sequential(\n",
              "        (0): Sequential(\n",
              "          (0): ConvTranspose2d(1024, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (2): Norm2d(\n",
              "            (ln): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "          )\n",
              "          (3): GELU(approximate='none')\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (2): Norm2d(\n",
              "            (ln): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "          )\n",
              "          (3): GELU(approximate='none')\n",
              "        )\n",
              "        (2): Sequential(\n",
              "          (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (2): Norm2d(\n",
              "            (ln): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "          )\n",
              "          (3): GELU(approximate='none')\n",
              "        )\n",
              "        (3): Sequential(\n",
              "          (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (2): Norm2d(\n",
              "            (ln): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "          )\n",
              "          (3): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (head): SegmentationHead(\n",
              "      (head): Sequential(\n",
              "        (0): Identity()\n",
              "        (1): Dropout(p=0.1, inplace=False)\n",
              "        (2): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (aux_heads): ModuleDict()\n",
              "    (neck): Sequential(\n",
              "      (0): SelectIndices()\n",
              "      (1): ReshapeTokensToImage()\n",
              "    )\n",
              "  )\n",
              "  (criterion): CrossEntropyLoss()\n",
              "  (train_metrics): MetricCollection(\n",
              "    (Multiclass_Accuracy): MulticlassAccuracy()\n",
              "    (Multiclass_Accuracy_Class): ClasswiseWrapper(\n",
              "      (metric): MulticlassAccuracy()\n",
              "    )\n",
              "    (Multiclass_F1_Score): MulticlassF1Score()\n",
              "    (Multiclass_Jaccard_Index): MulticlassJaccardIndex()\n",
              "    (Multiclass_Jaccard_Index_Class): ClasswiseWrapper(\n",
              "      (metric): MulticlassJaccardIndex()\n",
              "    )\n",
              "    (Multiclass_Jaccard_Index_Micro): MulticlassJaccardIndex(),\n",
              "    prefix=train/\n",
              "  )\n",
              "  (val_metrics): MetricCollection(\n",
              "    (Multiclass_Accuracy): MulticlassAccuracy()\n",
              "    (Multiclass_Accuracy_Class): ClasswiseWrapper(\n",
              "      (metric): MulticlassAccuracy()\n",
              "    )\n",
              "    (Multiclass_F1_Score): MulticlassF1Score()\n",
              "    (Multiclass_Jaccard_Index): MulticlassJaccardIndex()\n",
              "    (Multiclass_Jaccard_Index_Class): ClasswiseWrapper(\n",
              "      (metric): MulticlassJaccardIndex()\n",
              "    )\n",
              "    (Multiclass_Jaccard_Index_Micro): MulticlassJaccardIndex(),\n",
              "    prefix=val/\n",
              "  )\n",
              "  (test_metrics): ModuleList(\n",
              "    (0): MetricCollection(\n",
              "      (Multiclass_Accuracy): MulticlassAccuracy()\n",
              "      (Multiclass_Accuracy_Class): ClasswiseWrapper(\n",
              "        (metric): MulticlassAccuracy()\n",
              "      )\n",
              "      (Multiclass_F1_Score): MulticlassF1Score()\n",
              "      (Multiclass_Jaccard_Index): MulticlassJaccardIndex()\n",
              "      (Multiclass_Jaccard_Index_Class): ClasswiseWrapper(\n",
              "        (metric): MulticlassJaccardIndex()\n",
              "      )\n",
              "      (Multiclass_Jaccard_Index_Micro): MulticlassJaccardIndex(),\n",
              "      prefix=test/\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model_save_path  = Path(\"/content/drive/MyDrive/SCO_training/prithvi_state_dict.pt\")\n",
        "model.load_state_dict(torch.load(model_save_path))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "th54G6tBocez",
        "outputId": "e22a5727-1a99-4436-e176-bc9480d357e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total encoder parameter elements: 303886338\n"
          ]
        }
      ],
      "source": [
        "total_elements = sum(p.numel() for p in model.model.encoder.parameters())\n",
        "print(f\"Total encoder parameter elements: {total_elements}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tmIbTKoAvM5r"
      },
      "outputs": [],
      "source": [
        "def get_all_files(path, pattern='*', get_full_path=False):\n",
        "    files = list(pathlib.Path(path).glob(pattern))\n",
        "    if get_full_path:\n",
        "        onlyfiles = [os.path.join(path, f.name) for f in files if f.is_file()]\n",
        "    else:\n",
        "        onlyfiles = [f.name for f in files if f.is_file()]\n",
        "    return onlyfiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk-JmRPjocez",
        "outputId": "2afc576a-aed6-4a07-fa57-9d6933f42781"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "gt_paths = get_all_files(\n",
        "    '/content/drive/MyDrive/SCO_training/sentinel_images', '*.tif', True)\n",
        "len(gt_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyx2AOKgocez",
        "outputId": "a75196f1-4db2-4b03-c15d-1a26273886a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/drive/MyDrive/SCO_training/sentinel_images/2024.tif\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/osgeo/gdal.py:312: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4045505090.py:218: RuntimeWarning: invalid value encountered in cast\n",
            "  output[..., x] = band\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/SCO_training/footprints/2024.tif saved\n"
          ]
        }
      ],
      "source": [
        "dest = '/content/drive/MyDrive/SCO_training/footprints'\n",
        "create_dir_if_not_exists(dest)\n",
        "run_inference(gt_paths, dest, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqHTqPuHocez"
      },
      "outputs": [],
      "source": [
        "geoTiff_paths = iu.get_all_files(dest, '*.tif', True)\n",
        "len(geoTiff_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXu7qQQjoce0"
      },
      "outputs": [],
      "source": [
        "def merge_geoTiffs(geoTiff_paths, output_path, compression='NONE'):\n",
        "    \"\"\"\n",
        "    Merge multiple GeoTIFF files into a single GeoTIFF file.\n",
        "    \"\"\"\n",
        "    from osgeo import gdal\n",
        "\n",
        "    # Open each input GeoTIFF file read-only\n",
        "    geoTiffs = [gdal.Open(path, gdal.GA_ReadOnly) for path in geoTiff_paths]\n",
        "\n",
        "    # Option 1: pass format to gdal.Warp, pass a WarpOptions object via warpOptions\n",
        "    warp_options = gdal.WarpOptions(options=['COMPRESS=' + compression])\n",
        "    g = gdal.Warp(output_path, geoTiffs, format='GTiff', warpOptions=warp_options)\n",
        "    g = None  # Close dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMy7CviX6PTv"
      },
      "source": [
        "# Merge Predicted GeoTIFFs\n",
        "Combine multiple predicted GeoTIFF files into a single output for visualization or further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnMt7Jfzoce0"
      },
      "outputs": [],
      "source": [
        "merge_geoTiffs(\n",
        "    geoTiff_paths, 'C:\\\\Users\\\\emmanuelasare\\\\Downloads\\\\sentinel-aoi-2025(2025-7-22).tif',  'LZW')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}