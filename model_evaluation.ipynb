{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a0868d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54f11476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, struct, io, mmap\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def zero_pad_array(arr, target_hw):\n",
    "    th, tw = target_hw\n",
    "    h, w = arr.shape[:2]\n",
    "    pad_h = max(th - h, 0)\n",
    "    pad_w = max(tw - w, 0)\n",
    "    pad = ((pad_h // 2, pad_h - pad_h // 2),\n",
    "           (pad_w // 2, pad_w - pad_w // 2))\n",
    "    if arr.ndim == 3: pad += ((0, 0),)\n",
    "    return np.pad(arr, pad, mode='constant')\n",
    "\n",
    "class BurnScarTFRecordDataset(Dataset):\n",
    "    MEAN = np.array([1635.8452, 1584.4594, 1456.8425, 2926.6663, 2135.001, 1352.7313], dtype=np.float32)\n",
    "    STD  = np.array([884.3994, 815.4016, 839.0293, 1055.6382, 751.4628, 628.5323], dtype=np.float32)\n",
    "\n",
    "    _feature_desc = {                         \n",
    "        \"image_raw\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"mask_raw\":  tf.io.FixedLenFeature([], tf.string),\n",
    "        \"height\":    tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"width\":     tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"channels\":  tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"temporal_coords\": tf.io.FixedLenFeature([2], tf.float32),\n",
    "        \"location_coords\": tf.io.FixedLenFeature([2], tf.float32),\n",
    "    }\n",
    "\n",
    "    def __init__(self, tfrecord_file, transform=None, pad_to=(512, 512)):\n",
    "        super().__init__()\n",
    "        self.tfrecord_path = os.fspath(tfrecord_file)\n",
    "        self.transform = transform\n",
    "        self.pad_to = pad_to\n",
    "\n",
    "        # ---- build a list of byte offsets ----------------------------------\n",
    "        self._offsets = self._scan_index()\n",
    "        self._fh = open(self.tfrecord_path, 'rb')\n",
    "\n",
    "    def _scan_index(self):\n",
    "        \"\"\"Return a list with the starting byte of each record.\"\"\"\n",
    "        offsets = []\n",
    "        with open(self.tfrecord_path, 'rb') as f:\n",
    "            pos = 0\n",
    "            while True:\n",
    "                header = f.read(12)\n",
    "                if not header: break\n",
    "                rec_len = struct.unpack('<Q', header[:8])[0]\n",
    "                offsets.append(pos)\n",
    "                pos += 12 + rec_len + 4\n",
    "                f.seek(pos)\n",
    "        return offsets\n",
    "\n",
    "    def _read_record(self, offset):\n",
    "        \"\"\"Seek & return the serialised Example bytes of one record.\"\"\"\n",
    "        self._fh.seek(offset)\n",
    "        header = self._fh.read(12)\n",
    "        rec_len = struct.unpack('<Q', header[:8])[0]\n",
    "        data = self._fh.read(rec_len)\n",
    "        _ = self._fh.read(4)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._offsets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        serialised = self._read_record(self._offsets[idx])\n",
    "        ex = tf.io.parse_single_example(serialised, self._feature_desc)\n",
    "\n",
    "        h = int(ex[\"height\"])\n",
    "        w = int(ex[\"width\"])\n",
    "        c = int(ex[\"channels\"])\n",
    "\n",
    "        img = np.frombuffer(ex[\"image_raw\"].numpy(), dtype=np.float32).reshape((h, w, c))\n",
    "        msk = np.frombuffer(ex[\"mask_raw\"].numpy(),  dtype=np.uint8).reshape((h, w))\n",
    "\n",
    "        img = np.nan_to_num(img, nan=0.0)\n",
    "        msk = np.nan_to_num(msk.astype(np.float32), nan=0.0).astype(np.uint8)\n",
    "\n",
    "        img = (img - self.MEAN) / self.STD\n",
    "        img = zero_pad_array(img, self.pad_to)\n",
    "        msk = zero_pad_array(msk, self.pad_to)\n",
    "\n",
    "        temporal_coords = ex['temporal_coords'].numpy().astype(np.float32)   # (2,)\n",
    "        location_coords = ex['location_coords'].numpy().astype(np.float32)   # (2,)\n",
    "\n",
    "        temporal_coords = np.expand_dims(temporal_coords, axis=0)            # (1, 2)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img, mask=msk)\n",
    "            img, msk   = augmented[\"image\"], augmented[\"mask\"]\n",
    "        else:\n",
    "            img = torch.from_numpy(img.transpose(2, 0, 1))  # (C,H,W)\n",
    "            msk = torch.from_numpy(msk)\n",
    "\n",
    "        out = {\n",
    "            \"image\": img.float(),\n",
    "            \"temporal_coords\": torch.from_numpy(temporal_coords),\n",
    "            \"location_coords\": torch.from_numpy(location_coords),\n",
    "            \"mask\": msk.long()\n",
    "        }\n",
    "        return out\n",
    "\n",
    "    def __del__(self):\n",
    "        try:\n",
    "            if hasattr(self, '_fh') and not self._fh.closed:\n",
    "                self._fh.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.RandomRotate90(p=0.7),       # p: probability of applying this transform\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a1222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BurnScarTFRecordDataset(r\"Z:\\SPOT\\2023\\Asare\\ssm_scars_unseen_val.tfrecord\", transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d01f4475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emmanuelasare\\AppData\\Local\\anaconda3\\envs\\ml-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:root:Loaded weights for HLSBands.BLUE in position 0 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.GREEN in position 1 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.RED in position 2 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.NIR_NARROW in position 3 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.SWIR_1 in position 4 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.SWIR_2 in position 5 of patch embed\n"
     ]
    }
   ],
   "source": [
    "from terratorch.tasks import SemanticSegmentationTask\n",
    "\n",
    "model = SemanticSegmentationTask(\n",
    "    model_factory=\"EncoderDecoderFactory\",\n",
    "    model_args=dict(\n",
    "        backbone=\"terratorch_prithvi_eo_v2_300_tl\",\n",
    "        backbone_pretrained=True,\n",
    "        backbone_img_size=512,\n",
    "        backbone_bands=[\"BLUE\",\"GREEN\",\"RED\",\"NIR_NARROW\",\"SWIR_1\",\"SWIR_2\"],\n",
    "        necks=[{\"name\":\"SelectIndices\", \"indices\":[5,11,17,23]},\n",
    "               {\"name\":\"ReshapeTokensToImage\"}],\n",
    "        decoder=\"FCNDecoder\",              \n",
    "        decoder_channels=256,\n",
    "        num_classes=3,\n",
    "        head_dropout=0.1,\n",
    "    ),\n",
    "    loss=\"dice\",\n",
    "    freeze_backbone=False,\n",
    "    freeze_decoder=False,\n",
    "    optimizer=\"AdamW\",\n",
    "    lr=1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d0b240c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SemanticSegmentationTask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSemanticSegmentationTask\u001b[49m(\n\u001b[0;32m      2\u001b[0m     model_factory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoderDecoderFactory\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     model_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m      4\u001b[0m         backbone\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterratorch_prithvi_eo_v2_300_tl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m         backbone_pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      6\u001b[0m         backbone_img_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[0;32m      7\u001b[0m         backbone_bands\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLUE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGREEN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNIR_NARROW\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSWIR_1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSWIR_2\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      8\u001b[0m         necks\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      9\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelectIndices\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m17\u001b[39m, \u001b[38;5;241m23\u001b[39m]},\n\u001b[0;32m     10\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshapeTokensToImage\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     11\u001b[0m         ],\n\u001b[0;32m     12\u001b[0m         decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNetDecoder\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m         decoder_channels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m16\u001b[39m],   \u001b[38;5;66;03m# <--- THIS IS THE KEY\u001b[39;00m\n\u001b[0;32m     14\u001b[0m         num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     15\u001b[0m         head_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     16\u001b[0m     ),\n\u001b[0;32m     17\u001b[0m     freeze_backbone\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     18\u001b[0m     freeze_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     19\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdamW\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     20\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m,\n\u001b[0;32m     21\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SemanticSegmentationTask' is not defined"
     ]
    }
   ],
   "source": [
    "model = SemanticSegmentationTask(\n",
    "    model_factory=\"EncoderDecoderFactory\",\n",
    "    model_args=dict(\n",
    "        backbone=\"terratorch_prithvi_eo_v2_300_tl\",\n",
    "        backbone_pretrained=True,\n",
    "        backbone_img_size=512,\n",
    "        backbone_bands=[\"BLUE\",\"GREEN\",\"RED\",\"NIR_NARROW\",\"SWIR_1\",\"SWIR_2\"],\n",
    "        necks=[\n",
    "            {\"name\": \"SelectIndices\", \"indices\": [5, 11, 17, 23]},\n",
    "            {\"name\": \"ReshapeTokensToImage\"}\n",
    "        ],\n",
    "        decoder=\"UNetDecoder\",\n",
    "        decoder_channels=[256, 128, 64, 32, 16],   # <--- THIS IS THE KEY\n",
    "        num_classes=3,\n",
    "        head_dropout=0.1,\n",
    "    ),\n",
    "    freeze_backbone=False,\n",
    "    freeze_decoder=False,\n",
    "    optimizer=\"AdamW\",\n",
    "    lr=1e-4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36fb6b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"prithvi_state_dict_300m.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c552c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # pad x1 to match x2 size (H, W)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels=6, n_classes=1, bilinear=True):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        # Channels: [64, 128, 256, 512, 512, 512, 512, 512] is memory-friendly\n",
    "        self.inc    = DoubleConv(n_channels, 64)\n",
    "        self.down1  = Down(64, 128)\n",
    "        self.down2  = Down(128, 256)\n",
    "        self.down3  = Down(256, 512)\n",
    "        self.down4  = Down(512, 512)\n",
    "        self.down5  = Down(512, 512)\n",
    "        self.down6  = Down(512, 512)\n",
    "        self.down7  = Down(512, 512)\n",
    "\n",
    "        self.up1 = Up(512+512, 512, bilinear)\n",
    "        self.up2 = Up(512+512, 512, bilinear)\n",
    "        self.up3 = Up(512+512, 512, bilinear)\n",
    "        self.up4 = Up(512+512, 512, bilinear)\n",
    "        self.up5 = Up(512+256, 256, bilinear)\n",
    "        self.up6 = Up(256+128, 128, bilinear)\n",
    "        self.up7 = Up(128+64, 64, bilinear)\n",
    "\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)       # 64, 512, 512\n",
    "        x2 = self.down1(x1)    # 128, 256, 256\n",
    "        x3 = self.down2(x2)    # 256, 128, 128\n",
    "        x4 = self.down3(x3)    # 512, 64, 64\n",
    "        x5 = self.down4(x4)    # 512, 32, 32\n",
    "        x6 = self.down5(x5)    # 512, 16, 16\n",
    "        x7 = self.down6(x6)    # 512, 8, 8\n",
    "        x8 = self.down7(x7)    # 512, 4, 4\n",
    "\n",
    "        u1 = self.up1(x8, x7)  # 512, 8, 8\n",
    "        u2 = self.up2(u1, x6)  # 512, 16, 16\n",
    "        u3 = self.up3(u2, x5)  # 512, 32, 32\n",
    "        u4 = self.up4(u3, x4)  # 512, 64, 64\n",
    "        u5 = self.up5(u4, x3)  # 256, 128, 128\n",
    "        u6 = self.up6(u5, x2)  # 128, 256, 256\n",
    "        u7 = self.up7(u6, x1)  # 64, 512, 512\n",
    "\n",
    "        logits = self.outc(u7)\n",
    "        return logits\n",
    "\n",
    "# Example usage:\n",
    "# model = UNet(n_channels=6, n_classes=1)\n",
    "# x = torch.randn(1, 6, 512, 512)\n",
    "# out = model(x)\n",
    "# print(out.shape)  # Should print torch.Size([1, 1, 512, 512])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2663403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(n_channels=6, n_classes=3, bilinear=True)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2bca73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SemanticSegmentationTask(\n",
       "  (model): PixelWiseModel(\n",
       "    (encoder): PrithviViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv3d(6, 1024, kernel_size=(1, 16, 16), stride=(1, 16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (temporal_embed_enc): TemporalEncoder()\n",
       "      (location_embed_enc): LocationEncoder()\n",
       "      (blocks): ModuleList(\n",
       "        (0-23): 24 x Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): FCNDecoder(\n",
       "      (convs): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): ConvTranspose2d(1024, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (2): Norm2d(\n",
       "            (ln): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (3): GELU(approximate='none')\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (2): Norm2d(\n",
       "            (ln): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (3): GELU(approximate='none')\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (2): Norm2d(\n",
       "            (ln): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (3): GELU(approximate='none')\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (2): Norm2d(\n",
       "            (ln): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (3): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): SegmentationHead(\n",
       "      (head): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (aux_heads): ModuleDict()\n",
       "    (neck): Sequential(\n",
       "      (0): SelectIndices()\n",
       "      (1): ReshapeTokensToImage()\n",
       "    )\n",
       "  )\n",
       "  (criterion): DiceLoss()\n",
       "  (train_metrics): MetricCollection(\n",
       "    (Multiclass_Accuracy): MulticlassAccuracy()\n",
       "    (Multiclass_Accuracy_Class): ClasswiseWrapper(\n",
       "      (metric): MulticlassAccuracy()\n",
       "    )\n",
       "    (Multiclass_F1_Score): MulticlassF1Score()\n",
       "    (Multiclass_Jaccard_Index): MulticlassJaccardIndex()\n",
       "    (Multiclass_Jaccard_Index_Class): ClasswiseWrapper(\n",
       "      (metric): MulticlassJaccardIndex()\n",
       "    )\n",
       "    (Multiclass_Jaccard_Index_Micro): MulticlassJaccardIndex(),\n",
       "    prefix=train/\n",
       "  )\n",
       "  (val_metrics): MetricCollection(\n",
       "    (Multiclass_Accuracy): MulticlassAccuracy()\n",
       "    (Multiclass_Accuracy_Class): ClasswiseWrapper(\n",
       "      (metric): MulticlassAccuracy()\n",
       "    )\n",
       "    (Multiclass_F1_Score): MulticlassF1Score()\n",
       "    (Multiclass_Jaccard_Index): MulticlassJaccardIndex()\n",
       "    (Multiclass_Jaccard_Index_Class): ClasswiseWrapper(\n",
       "      (metric): MulticlassJaccardIndex()\n",
       "    )\n",
       "    (Multiclass_Jaccard_Index_Micro): MulticlassJaccardIndex(),\n",
       "    prefix=val/\n",
       "  )\n",
       "  (test_metrics): ModuleList(\n",
       "    (0): MetricCollection(\n",
       "      (Multiclass_Accuracy): MulticlassAccuracy()\n",
       "      (Multiclass_Accuracy_Class): ClasswiseWrapper(\n",
       "        (metric): MulticlassAccuracy()\n",
       "      )\n",
       "      (Multiclass_F1_Score): MulticlassF1Score()\n",
       "      (Multiclass_Jaccard_Index): MulticlassJaccardIndex()\n",
       "      (Multiclass_Jaccard_Index_Class): ClasswiseWrapper(\n",
       "        (metric): MulticlassJaccardIndex()\n",
       "      )\n",
       "      (Multiclass_Jaccard_Index_Micro): MulticlassJaccardIndex(),\n",
       "      prefix=test/\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"prithvi_state_dict_unet.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baed90de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall(ground_truth_mask, predicted_mask, num_classes):\n",
    "    # print(ground_truth_mask.shape, predicted_mask.shape)\n",
    "    precision = [None] * num_classes  # np.zeros(num_classes)\n",
    "    recall = [None] * num_classes  # np.zeros(num_classes)\n",
    "    precision_dict = dict()\n",
    "    recall_dict = dict()\n",
    "    for class_id in range(num_classes):\n",
    "        true_positives = np.sum(\n",
    "            (ground_truth_mask == class_id) & (predicted_mask == class_id))\n",
    "        false_positives = np.sum(\n",
    "            (ground_truth_mask != class_id) & (predicted_mask == class_id))\n",
    "        false_negatives = np.sum(\n",
    "            (ground_truth_mask == class_id) & (predicted_mask != class_id))\n",
    "        p = None\n",
    "        r = None\n",
    "        if true_positives + false_positives != 0:\n",
    "            p = true_positives / (true_positives + false_positives)\n",
    "        if true_positives + false_negatives != 0:\n",
    "            r = true_positives / (true_positives + false_negatives)\n",
    "        precision_dict[class_id] = p\n",
    "        recall_dict[class_id] = r\n",
    "    return precision_dict, recall_dict\n",
    "\n",
    "\n",
    "def calc_f1_score(precision, recall):\n",
    "    if precision is None or recall is None or (precision + recall) == 0:\n",
    "        return None\n",
    "    return (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def calculate_raw_scores(ground_truth_mask, predicted_mask, num_classes):\n",
    "    # print(ground_truth_mask.shape, predicted_mask.shape)\n",
    "    # precision = [None] * num_classes  # np.zeros(num_classes)\n",
    "    # recall = [None] * num_classes  # np.zeros(num_classes)\n",
    "    # precision_dict = dict()\n",
    "    # recall_dict = dict()\n",
    "    true_positives_dict = dict()\n",
    "    false_positives_dict = dict()\n",
    "    false_negatives_dict = dict()\n",
    "    for class_id in range(num_classes):\n",
    "        true_positives = np.sum(\n",
    "            (ground_truth_mask == class_id) & (predicted_mask == class_id))\n",
    "        false_positives = np.sum(\n",
    "            (ground_truth_mask != class_id) & (predicted_mask == class_id))\n",
    "        false_negatives = np.sum(\n",
    "            (ground_truth_mask == class_id) & (predicted_mask != class_id))\n",
    "        true_positives_dict[class_id] = true_positives\n",
    "        false_positives_dict[class_id] = false_positives\n",
    "        false_negatives_dict[class_id] = false_negatives\n",
    "        # p = None\n",
    "        # r = None\n",
    "        # if true_positives + false_positives != 0:\n",
    "        #     p = true_positives / (true_positives + false_positives)\n",
    "        # if true_positives + false_negatives != 0:\n",
    "        #     r = true_positives / (true_positives + false_negatives)\n",
    "        # precision_dict[class_id] = p\n",
    "        # recall_dict[class_id] = r\n",
    "    return true_positives_dict, false_positives_dict, false_negatives_dict\n",
    "\n",
    "def calculate_mean_precision_recall(model, val_datagen, num_classes):\n",
    "    # Replace with the actual number of classes in your model\n",
    "    # num_classes = 3\n",
    "    # precision_total_dict = dict()\n",
    "    # recall_total_dict = dict()\n",
    "    total_true_positives_dict = dict()\n",
    "    total_false_positives_dict = dict()\n",
    "    total_false_negatives_dict = dict()\n",
    "    precision_scores = np.zeros(num_classes)\n",
    "    recall_scores = np.zeros(num_classes)\n",
    "    f1_scores = np.zeros(num_classes)\n",
    "    # class_iou_scores = np.zeros(num_classes)\n",
    "    # class_counts = np.zeros(num_classes)\n",
    "\n",
    "    # Iterate over the validation data generator\n",
    "    for images, labels in val_datagen:\n",
    "        for image, label in zip(images, labels):\n",
    "            # Predict the labels using the model\n",
    "            predictions = model.predict(np.array([image]), verbose=False)\n",
    "            predicted_classes = np.argmax(predictions, axis=3)\n",
    "            # print(predicted_classes.shape)\n",
    "            true_positives_dict, false_positives_dict, false_negatives_dict = calculate_raw_scores(\n",
    "                label, predicted_classes[0], num_classes)\n",
    "            # print(precision_dict, recall_dict)\n",
    "            for class_id in range(num_classes):\n",
    "                # p = precision_dict.get(class_id)\n",
    "                # r = recall_dict.get(class_id)\n",
    "                ttp = total_true_positives_dict.get(class_id, 0.0)\n",
    "                tfp = total_false_positives_dict.get(class_id, 0.0)\n",
    "                tfn = total_false_negatives_dict.get(class_id, 0.0)\n",
    "                ttp += true_positives_dict[class_id]\n",
    "                tfp += false_positives_dict[class_id]\n",
    "                tfn += false_negatives_dict[class_id]\n",
    "                total_true_positives_dict[class_id] = ttp\n",
    "                total_false_positives_dict[class_id] = tfp\n",
    "                total_false_negatives_dict[class_id] = tfn\n",
    "                # if p != None:\n",
    "                #     pt = precision_total_dict.get(class_id, [0, 0])\n",
    "                #     pt[0] += p\n",
    "                #     pt[1] += 1\n",
    "                #     precision_total_dict[class_id] = pt\n",
    "                # if r != None:\n",
    "                #     rt = recall_total_dict.get(class_id, [0, 0])\n",
    "                #     rt[0] += r\n",
    "                #     rt[1] += 1\n",
    "                #     recall_total_dict[class_id] = rt\n",
    "        # Compute IoU scores for each class\n",
    "        # for class_id in range(num_classes):\n",
    "\n",
    "    # Calculate average IoU scores for each class\n",
    "    # class_iou_scores /= class_counts\n",
    "    for class_id in range(num_classes):\n",
    "        ttp = total_true_positives_dict.get(class_id)\n",
    "        tfp = total_false_positives_dict.get(class_id)\n",
    "        tfn = total_false_negatives_dict.get(class_id)\n",
    "        p = None\n",
    "        r = None\n",
    "        if ttp + tfp != 0:\n",
    "            p = ttp / (ttp + tfp)\n",
    "        if ttp + tfn != 0:\n",
    "            r = ttp / (ttp + tfn)\n",
    "        # pt = precision_total_dict.get(x)\n",
    "        # rt = recall_total_dict.get(x)\n",
    "        # total_score = pt[0]\n",
    "        precision_scores[class_id] = p\n",
    "        # rt = recall_total_dict.get(x)\n",
    "        # total_score = pt[0]\n",
    "        recall_scores[class_id] = r\n",
    "        f1_scores[class_id] = calc_f1_score(p, r)\n",
    "\n",
    "    # Create a DataFrame with class labels and corresponding IoU scores\n",
    "    iou_df = pd.DataFrame(\n",
    "        {'class_val': range(num_classes), 'precision': precision_scores, 'recall': recall_scores, 'f1 score': f1_scores})\n",
    "    # print(precision_total_dict)\n",
    "    return iou_df\n",
    "\n",
    "\n",
    "def calculate_mean_precision_recall(model_func, model, val_datagen, num_classes, device):\n",
    "    total_true_positives_dict = {c: 0 for c in range(num_classes)}\n",
    "    total_false_positives_dict = {c: 0 for c in range(num_classes)}\n",
    "    total_false_negatives_dict = {c: 0 for c in range(num_classes)}\n",
    "\n",
    "    # For batchwise prediction, create predictor\n",
    "    predict = model_func(model, device)\n",
    "\n",
    "    for batch in val_datagen:\n",
    "        # images: batch of shape [B, C, H, W] (torch.Tensor or np.ndarray)\n",
    "        # labels: batch of shape [B, H, W] (numpy or torch)\n",
    "        images = batch['image'].to(device)\n",
    "        masks = batch['mask'].to(device)\n",
    "        temporal_coords=batch[\"temporal_coords\"].to(device)\n",
    "        location_coords=batch[\"location_coords\"].to(device)\n",
    "        # if torch.is_tensor(images):\n",
    "        #     images = images.to(device)\n",
    "        pred_masks = predict(images, temporal_coords=temporal_coords, location_coords=location_coords)  # shape: [B, H, W] (np.ndarray)\n",
    "        if torch.is_tensor(masks):\n",
    "            masks = masks.cpu().numpy()\n",
    "        for pred_mask, label in zip(pred_masks, masks):\n",
    "            true_positives_dict, false_positives_dict, false_negatives_dict = calculate_raw_scores(\n",
    "                label, pred_mask, num_classes)\n",
    "            for class_id in range(num_classes):\n",
    "                total_true_positives_dict[class_id] += true_positives_dict[class_id]\n",
    "                total_false_positives_dict[class_id] += false_positives_dict[class_id]\n",
    "                total_false_negatives_dict[class_id] += false_negatives_dict[class_id]\n",
    "\n",
    "    # Aggregate results\n",
    "    precision_scores = np.zeros(num_classes)\n",
    "    recall_scores = np.zeros(num_classes)\n",
    "    f1_scores = np.zeros(num_classes)\n",
    "    iou_scores = np.zeros(num_classes)\n",
    "\n",
    "    for class_id in range(num_classes):\n",
    "        ttp = total_true_positives_dict[class_id]\n",
    "        tfp = total_false_positives_dict[class_id]\n",
    "        tfn = total_false_negatives_dict[class_id]\n",
    "        denom = ttp + tfp + tfn\n",
    "        iou = ttp / denom if denom != 0 else None\n",
    "        p = ttp / (ttp + tfp) if (ttp + tfp) != 0 else None\n",
    "        r = ttp / (ttp + tfn) if (ttp + tfn) != 0 else None\n",
    "        f1 = calc_f1_score(p, r)\n",
    "        precision_scores[class_id] = p if p is not None else np.nan\n",
    "        recall_scores[class_id] = r if r is not None else np.nan\n",
    "        f1_scores[class_id] = f1 if f1 is not None else np.nan\n",
    "        iou_scores[class_id] = iou if iou is not None else np.nan\n",
    "\n",
    "    iou_df = pd.DataFrame({\n",
    "        'class_val': range(num_classes),\n",
    "        'iou': iou_scores,\n",
    "        'precision': precision_scores,\n",
    "        'recall': recall_scores,\n",
    "        'f1 score': f1_scores\n",
    "    })\n",
    "    return iou_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c5e4931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def model_predict(model, device):\n",
    "    \"\"\"\n",
    "    Returns a function that predicts class masks from input tensors using a model.\n",
    "    Handles both single image and batch input.\n",
    "    \"\"\"\n",
    "    def predict(images, temporal_coords, location_coords):\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if isinstance(images, np.ndarray):\n",
    "                images = torch.from_numpy(images)\n",
    "            images = images.to(device)\n",
    "            if images.dim() == 3:  # Single image, shape: [C, H, W]\n",
    "                images = images.unsqueeze(0)\n",
    "            outputs = model(images).output  # [B, num_classes, H, W]\n",
    "            pred_masks = torch.argmax(outputs, dim=1)  # [B, H, W]\n",
    "            pred_masks = pred_masks.cpu().numpy()\n",
    "        return pred_masks\n",
    "    return predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d2ba9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(model, device):\n",
    "    \"\"\"\n",
    "    Returns a function that predicts class masks from input tensors using a model.\n",
    "    Handles both single image and batch input.\n",
    "    Supports optional temporal_coords and location_coords.\n",
    "    \"\"\"\n",
    "    def predict(images, temporal_coords=None, location_coords=None):\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if isinstance(images, np.ndarray):\n",
    "                images = torch.from_numpy(images)\n",
    "            images = images.to(device)\n",
    "            if images.dim() == 3:  # Single image, shape: [C, H, W]\n",
    "                images = images.unsqueeze(0)\n",
    "\n",
    "            # Prepare optional kwargs\n",
    "            kwargs = {}\n",
    "            if temporal_coords is not None:\n",
    "                if isinstance(temporal_coords, np.ndarray):\n",
    "                    temporal_coords = torch.from_numpy(temporal_coords)\n",
    "                kwargs[\"temporal_coords\"] = temporal_coords.to(device)\n",
    "            if location_coords is not None:\n",
    "                if isinstance(location_coords, np.ndarray):\n",
    "                    location_coords = torch.from_numpy(location_coords)\n",
    "                kwargs[\"location_coords\"] = location_coords.to(device)\n",
    "\n",
    "            outputs = model(images, **kwargs).output  # [B, num_classes, H, W]\n",
    "            pred_masks = torch.argmax(outputs, dim=1)  # [B, H, W]\n",
    "            pred_masks = pred_masks.cpu().numpy()\n",
    "        return pred_masks\n",
    "    return predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feb058be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58e08fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class_val       iou  precision    recall  f1 score\n",
      "0          0  0.987966   0.991099  0.996810  0.993947\n",
      "1          1  0.666150   0.880803  0.732152  0.799628\n",
      "2          2  0.303249   0.511626  0.426792  0.465374\n"
     ]
    }
   ],
   "source": [
    "# Suppose you have:\n",
    "# val_datagen: yields (images, labels) where images.shape = [B, C, H, W], labels.shape = [B, H, W]\n",
    "# model: your PyTorch segmentation model\n",
    "# device: torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# num_classes: int\n",
    "\n",
    "# Pass the predictor factory:\n",
    "results_df = calculate_mean_precision_recall(model_predict, model, train_loader, 3, device)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "400b6e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class_val       iou  precision    recall  f1 score\n",
      "0          0  0.988314   0.991923  0.996332  0.994122\n",
      "1          1  0.678351   0.870483  0.754503  0.808354\n",
      "2          2  0.307280   0.472827  0.467415  0.470105\n"
     ]
    }
   ],
   "source": [
    "# Pass the predictor factory:\n",
    "results_df = calculate_mean_precision_recall(model_predict, model, train_loader, 3, device)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "777f9949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class_val       iou  precision    recall  f1 score\n",
      "0          0  0.988514   0.992639  0.995814  0.994224\n",
      "1          1  0.688667   0.858037  0.777224  0.815634\n",
      "2          2  0.308345   0.489770  0.454268  0.471352\n"
     ]
    }
   ],
   "source": [
    "# Suppose you have:\n",
    "# val_datagen: yields (images, labels) where images.shape = [B, C, H, W], labels.shape = [B, H, W]\n",
    "# model: your PyTorch segmentation model\n",
    "# device: torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# num_classes: int\n",
    "\n",
    "# Pass the predictor factory:\n",
    "results_df = calculate_mean_precision_recall(model_predict, model, train_loader, 3, device)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37b2088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class_val       iou  precision    recall  f1 score\n",
      "0          0  0.997108   0.998244  0.998860  0.998552\n",
      "1          1  0.834184   0.927985  0.891923  0.909597\n",
      "2          2  0.834855   0.918178  0.901958  0.909996\n"
     ]
    }
   ],
   "source": [
    "# Pass the predictor factory:\n",
    "#Val\n",
    "results_df = calculate_mean_precision_recall(model_predict, model, train_loader, 3, device)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bfb148e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class_val       iou  precision    recall  f1 score\n",
      "0          0  0.997204   0.998299  0.998901  0.998600\n",
      "1          1  0.837536   0.929852  0.894024  0.911586\n",
      "2          2  0.843570   0.922625  0.907793  0.915149\n"
     ]
    }
   ],
   "source": [
    "# Pass the predictor factory:\n",
    "results_df = calculate_mean_precision_recall(model_predict, model, train_loader, 3, device)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5fd396d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class_val       iou  precision    recall  f1 score\n",
      "0          0  0.997761   0.998847  0.998912  0.998879\n",
      "1          1  0.871322   0.933205  0.929277  0.931237\n",
      "2          2  0.869823   0.930450  0.930310  0.930380\n"
     ]
    }
   ],
   "source": [
    "# Pass the predictor factory:\n",
    "#Val\n",
    "results_df = calculate_mean_precision_recall(model_predict, model, train_loader, 3, device)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55274e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn val\n",
      "   class_val       iou  precision    recall  f1 score\n",
      "0          0  0.994738   0.996022  0.998706  0.997362\n",
      "1          1  0.698693   0.901775  0.756248  0.822625\n",
      "2          2  0.676976   0.899543  0.732342  0.807377\n"
     ]
    }
   ],
   "source": [
    "# Pass the predictor factory:\n",
    "#Val\n",
    "results_df = calculate_mean_precision_recall(model_predict, model, train_loader, 3, device)\n",
    "print('cnn val')\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2b8227b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn train\n",
      "   class_val       iou  precision    recall  f1 score\n",
      "0          0  0.994855   0.996136  0.998709  0.997421\n",
      "1          1  0.702235   0.901574  0.760541  0.825074\n",
      "2          2  0.689044   0.901299  0.745280  0.815898\n"
     ]
    }
   ],
   "source": [
    "#Val\n",
    "results_df = calculate_mean_precision_recall(model_predict, model, train_loader, 3, device)\n",
    "print('cnn train')\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "663092e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn val\n",
      "   class_val       iou  precision    recall  f1 score\n",
      "0          0  0.996057   0.998178  0.997872  0.998025\n",
      "1          1  0.785465   0.871080  0.888785  0.879844\n",
      "2          2  0.778125   0.877069  0.873379  0.875220\n"
     ]
    }
   ],
   "source": [
    "# Pass the predictor factory:\n",
    "#Val\n",
    "results_df = calculate_mean_precision_recall(model_predict, model, train_loader, 3, device)\n",
    "print('cnn val')\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca891a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "def extract_year_from_filename(filename):\n",
    "    match = re.search(r\"(20\\d{2})\", filename)\n",
    "    return int(match.group(1)) if match else 2023\n",
    "\n",
    "def get_lat_lon_from_metadata(meta):\n",
    "    try:\n",
    "        bounds = meta['bounds']\n",
    "        lat = (bounds.top + bounds.bottom) / 2\n",
    "        lon = (bounds.left + bounds.right) / 2\n",
    "        return lat, lon\n",
    "    except Exception:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "def preprocess(img):\n",
    "    mean = np.array([1635.8452, 1584.4594, 1456.8425, 2926.6663, 2135.001, 1352.7313], dtype=np.float32)\n",
    "    std  = np.array([884.3994, 815.4016, 839.0293, 1055.6382, 751.4628, 628.5323], dtype=np.float32)\n",
    "    img = (img - mean[:, None, None]) / std[:, None, None]\n",
    "    return torch.from_numpy(img).float()\n",
    "\n",
    "def process_geotiff_patchwise(\n",
    "    real_path, mask_path, model, out_fp_path, out_fn_path,\n",
    "    patch_size=512, preprocess_fn=None, device='cuda'\n",
    "):\n",
    "    with rasterio.open(real_path) as src:\n",
    "        meta = src.meta.copy()\n",
    "        H, W = src.height, src.width\n",
    "        C = src.count\n",
    "        lat, lon = get_lat_lon_from_metadata(src)\n",
    "\n",
    "        # --- Load mask and resize to match image shape ---\n",
    "        mask = np.array(Image.open(mask_path))\n",
    "        if mask.shape != (H, W):\n",
    "            mask = np.array(Image.fromarray(mask).resize((W, H), Image.NEAREST))\n",
    "\n",
    "        # --- Prepare output arrays ---\n",
    "        fp_full = np.zeros((H, W), dtype=np.uint8)\n",
    "        fn_full = np.zeros((H, W), dtype=np.uint8)\n",
    "\n",
    "        # --- Prepare temporal/location coords ---\n",
    "        fname = os.path.splitext(os.path.basename(real_path))[0]\n",
    "        year = extract_year_from_filename(fname)\n",
    "        doy = 1\n",
    "\n",
    "        # --- Sliding window over the image ---\n",
    "        step = patch_size\n",
    "        for row in range(0, H, step):\n",
    "            for col in range(0, W, step):\n",
    "                win_h = min(step, H - row)\n",
    "                win_w = min(step, W - col)\n",
    "\n",
    "                # Read patch\n",
    "                img_patch = src.read(window=Window(col, row, win_w, win_h)).astype(np.float32)  # (C, h, w)\n",
    "                # Mask patch\n",
    "                mask_patch = mask[row:row+win_h, col:col+win_w]\n",
    "\n",
    "                # Zero pad if needed\n",
    "                pad_h = patch_size - win_h\n",
    "                pad_w = patch_size - win_w\n",
    "                if pad_h > 0 or pad_w > 0:\n",
    "                    img_patch = np.pad(img_patch, ((0,0), (0,pad_h), (0,pad_w)), mode='constant')\n",
    "                    mask_patch = np.pad(mask_patch, ((0,pad_h), (0,pad_w)), mode='constant')\n",
    "\n",
    "                # Preprocess and model inference\n",
    "                img_input = preprocess_fn(img_patch) if preprocess_fn else torch.from_numpy(img_patch)\n",
    "                img_input = img_input.unsqueeze(0).to(device)\n",
    "\n",
    "                temporal_coords = torch.tensor([[year, doy]], dtype=torch.float32).unsqueeze(1).to(device)\n",
    "                location_coords = torch.tensor([[lat, lon]], dtype=torch.float32).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(\n",
    "                        img_input,\n",
    "                        temporal_coords=temporal_coords,\n",
    "                        location_coords=location_coords,\n",
    "                    )\n",
    "                    logits = outputs.output if hasattr(outputs, \"output\") else outputs\n",
    "                    pred_mask_patch = torch.argmax(logits, dim=1).cpu().numpy()[0]  # (h, w)\n",
    "\n",
    "                # Crop back to original (non-padded) shape\n",
    "                pred_mask_patch = pred_mask_patch[:win_h, :win_w]\n",
    "                mask_patch = mask_patch[:win_h, :win_w]\n",
    "\n",
    "                # Compute FP/FN patch\n",
    "                fp_patch = ((pred_mask_patch != mask_patch) & (pred_mask_patch != 0)) * pred_mask_patch\n",
    "                fn_patch = ((pred_mask_patch != mask_patch) & (mask_patch != 0)) * mask_patch\n",
    "\n",
    "                # Paste patch into full output arrays\n",
    "                fp_full[row:row+win_h, col:col+win_w] = fp_patch\n",
    "                fn_full[row:row+win_h, col:col+win_w] = fn_patch\n",
    "\n",
    "        # Write output GeoTIFFs\n",
    "        meta.update(dtype=rasterio.uint8, count=1)\n",
    "        with rasterio.open(out_fp_path, \"w\", **meta) as dst:\n",
    "            dst.write(fp_full, 1)\n",
    "        with rasterio.open(out_fn_path, \"w\", **meta) as dst:\n",
    "            dst.write(fn_full, 1)\n",
    "\n",
    "def batch_generate_fp_fn(\n",
    "    real_dir, mask_dir, model, out_dir, patch_size=512,\n",
    "    preprocess_fn=None, device='cuda'\n",
    "):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    real_files = sorted([f for f in os.listdir(real_dir) if f.endswith(\".tif\")])\n",
    "\n",
    "    for real_fname in real_files:\n",
    "        real_path = os.path.join(real_dir, real_fname)\n",
    "        mask_path = os.path.join(mask_dir, os.path.splitext(real_fname)[0] + \".png\")\n",
    "        if not os.path.exists(mask_path):\n",
    "            print(f\"Missing mask for {real_fname}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        fp_path = os.path.join(out_dir, os.path.splitext(real_fname)[0] + \"_fp.tif\")\n",
    "        fn_path = os.path.join(out_dir, os.path.splitext(real_fname)[0] + \"_fn.tif\")\n",
    "        process_geotiff_patchwise(\n",
    "            real_path, mask_path, model, fp_path, fn_path,\n",
    "            patch_size=patch_size, preprocess_fn=preprocess_fn, device=device\n",
    "        )\n",
    "        print(f\"Processed {real_fname}\")\n",
    "\n",
    "# Example usage:\n",
    "# batch_generate_fp_fn(\n",
    "#     real_dir=\"real\",\n",
    "#     mask_dir=\"mask\",\n",
    "#     model=your_model,\n",
    "#     out_dir=\"fpfn_output\",\n",
    "#     patch_size=512,\n",
    "#     preprocess_fn=preprocess,\n",
    "#     device='cuda'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "897651c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed sentinel_h_ssm_aoi_2019.tif\n",
      "Processed sentinel_sr_h_ssm_aoi_2019.tif\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "batch_generate_fp_fn(\n",
    "    real_dir=r\"D:\\sentinel_ssm\\data\\segmentation\\original_val\\real\",\n",
    "    mask_dir=r\"D:\\sentinel_ssm\\data\\segmentation\\original_val\\mask\",\n",
    "    model=model,\n",
    "    out_dir=r\"D:\\sentinel_ssm\\data\\segmentation\\fpfn\",\n",
    "    patch_size=512,\n",
    "    preprocess_fn=preprocess,\n",
    "    device='cuda'\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
